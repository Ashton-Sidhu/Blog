{
  
    
        "post0": {
            "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
            "content": "Prefect is a workflow management system that enables users to easily take data applications and add retries, logging, dynamic mapping, caching, failure notifications, scheduling and more — all with functional Python API. Prefect allows users to take their existing code and transform it into a DAG (Directed Acyclic Graph) with dependencies already identified [1]. It simplifies the creation of ETL pipelines and dependencies and enables users to strictly focus on the application code instead of the pipeline code (looking at you Airflow). Prefect can even create distributed pipelines to parallelize your data applications. . Databricks at its core is a PaaS (Platform as a Service) that delivers fully managed Spark clusters, interactive &amp; collaborative notebooks (similar to Jupyter), a production pipeline scheduler and a platform for powering your Spark-based applications. It is integrated in both the Azure and AWS ecosystem to make working with big data simple. Databricks enables users to run their custom Spark applications on their managed Spark clusters. It even allows users to schedule their notebooks as Spark jobs. It has completely simplified big data development and the ETL process surrounding it. . Databricks has become such an integral big data ETL tool, one that I use every day at work, so I made a contribution to the Prefect project enabling users to integrate Databricks jobs with Prefect. In this tutorial we will go over just that — how you can incorporate running Databricks notebooks and Spark jobs in your Prefect flows. . Prerequisites . There is no prior knowledge needed for this post however a free Prefect account is recommended to implement the example. While this post will touch on Prefect basics, it is not an in depth Prefect tutorial. . Prefect Basics . Tasks . A task in Prefect is the equivalent of a step in your data pipeline. It is as simple as a Python function in your application or script. There are no restrictions on how simple or complex tasks can be. That being said, it’s best to follow coding best practices and develop your functions, so they only do one thing. Prefect themselves recommend this. . . In general, we encourage small tasks over monolithic ones, each task should perform a discrete logical step of your workflow, but not more. [2] By keeping tasks small, you will get the most out of Prefect’s engine such as efficient state checkpoints. . Flows . A flow is what ties all your tasks and their dependencies together. It describes dependencies between tasks, their ordering and the data flow. Flows pull together tasks and make it into a pipeline rounding out your data application. . . Native Databricks Integration in Prefect . I made a contribution to the Prefect project by the implementing the tasks DatabricksRunNow &amp; DatabricksRunSubmit enabling seamless integration between Prefect and Databricks. Through these tasks users can externally trigger a defined Databricks job or a single run of a jar, Python script or notebook. Once a task has been executed it uses Databricks native API calls to run notebooks or Spark Jobs. When the task is running it will continue to poll the current status of the run until it’s completed. Once a task is completed it will allow for downstream tasks to run if it is successful. . Creating a Flow with Databricks Tasks . Before we get started writing any code, we have to create a Prefect Secret that is going to store our Databricks connection string. From your Prefect Cloud account, click on Team from the left side menu and go to the Secrets section. This section is where you manage all the secrets for your Prefect Flows. . To generate the Databricks connection string you will need the host name of your Databricks instance as well as a PAT for your Databricks account. To create a Databricks PAT, follow these steps from the Databricks documentation. The connection string has to be a valid JSON object. The title of the secret has to be DATABRICKS_CONNECTION_STRING. . . Creating the Tasks . Let’s start our flow by defining some common tasks that we will need to run our Databricks notebooks and Spark jobs. . from prefect import task, Flow from prefect.tasks.databricks.databricks_submitjob import ( DatabricksRunNow, DatabricksSubmitRun, ) from prefect.tasks.secrets.base import PrefectSecret conn = PrefectSecret(&quot;DATABRICKS_CONNECTION_STRING&quot;) # Initialize Databricks task class as a template # We will then use the task function to pass in unique config options &amp; params RunNow = DatabricksRunNow(conn) SubmitRun = DatabricksSubmitRun(conn) . We define two task objects, RunNow and SubmitRun, to act as templates to run our Databricks jobs. We can reuse these same tasks with different configurations to easily create new Databricks jobs. Let’s create some helper tasks to dynamically create the configuration of our jobs. . @task def get_submit_config(python_params: list): &quot;&quot;&quot; SubmitRun config template for the DatabricksSubmitRun task, Spark Python Task params must be passed as a list. &quot;&quot;&quot; return { &quot;run_name&quot;: &quot;MyDatabricksJob&quot;, &quot;new_cluster&quot;: { &quot;spark_version&quot;: &quot;7.3.x-scala2.12&quot;, &quot;node_type_id&quot;: &quot;r3.xlarge&quot;, &quot;aws_attributes&quot;: { &quot;availability&quot;: &quot;ON_DEMAND&quot; }, &quot;num_workers&quot;: 10 }, &quot;spark_python_task&quot;: { &quot;python_file&quot;: &quot;/Users/ashton/databricks_task/main.py&quot;, &quot;parameters&quot;: python_params, }, } @task def get_run_now_config(notebook_params: dict): &quot;&quot;&quot; RunNow config template for the DatabricksSubmitRun task, Notebook Task params must be passed as a dictionary. &quot;&quot;&quot; return {&quot;job_id&quot;: 42, &quot;notebook_params&quot;: notebook_params} . The get_submit_config task allows us to dynamically pass parameters to a Python script that is on DBFS (Databricks File System) and return a configuration to run a single use Databricks job. You can add more flexibility by creating more parameters that map to configuration options in your Databricks job configuration. The get_run_now_config executes same task except it returns a configuration for the DatabricksRunNow task to run a preconfigured Databricks Notebooks job. The schemas of both the get_run_now_config and get_submit_config match the Run Now and Runs Submit API respectively. . . Python file parameters must be passed as a list and Notebook parameters must be passed as a dictionary. Now let’s create a flow that can run our tasks. . Creating the Flow . We’re going to create a flow that runs a preconfigured notebook job on Databricks, followed by two subsequent Python script jobs. . with Flow(&quot;Databricks-Tasks&quot;, schedule=None) as flow: run_now_config = get_run_now_config({&quot;param1&quot;: &quot;value&quot;}) submit_config_a = get_submit_config([&quot;param1&quot;]) submit_config_b = get_submit_config([&quot;param2&quot;]) run_now_task = RunNow(json=run_now_config) submit_task_a = SubmitRun(json=submit_config_a) submit_task_b = SubmitRun(json=submit_config_b) # Since Databricks tasks don&#39;t return any data dependencies we can leverage, # we have to define the dependencies between Databricks tasks themselves flow.add_edge(run_now_task, submit_task_a) flow.add_edge(submit_task_a, submit_task_b) . We first need to create the Databricks job configuration by using our get_run_now_config and get_submit_config tasks. Pass the run now configuration to the RunNow task and the submit run configuration to the SubmitRun task through the json argument. The json parameter takes in a dictionary that matches the Run Now and Submit Run APIs mentioned above. To run more Databricks jobs we instantiate either the RunNow or SubmitRun templates we created and pass in a new json job config. . One of the awesome features of a Prefect flow is that it automatically builds a DAG from your tasks. It looks at task inputs as data dependencies and from that, can infer what tasks need to be completed before other tasks can run. For example, since our run_now_task has the input run_now_config, the flow builds the DAG knowing the get_run_now_config task has to run before the run_now_task. . Some tasks don’t return data that can be used as inputs in down stream tasks. For example, the Databricks tasks only return a job ID. We can still define the inter-task dependencies of the flow by using the .add_edge function. This will add dependencies between tasks that aren’t used as inputs for further down stream tasks. For example, flow.add_edge(run_now_task, submit_task_a) says that submit_task_a is a downstream task from the run_now_task and that submit_task_a cannot run until the run_now_task has been completed. By adding the edges to the remaining Databricks task we get our final flow, which you can also view in the Prefect schematics tab. . . To the run the flow, we call the .run() method of our flow object — flow.run(). The final flow then looks like this: . from prefect import task, Flow from prefect.tasks.databricks.databricks_submitjob import ( DatabricksRunNow, DatabricksSubmitRun, ) from prefect.tasks.secrets.base import PrefectSecret @task def get_submit_config(python_params: list): &quot;&quot;&quot; SubmitRun config template for the DatabricksSubmitRun task, Spark Python Task params must be passed as a list. &quot;&quot;&quot; return { &quot;run_name&quot;: &quot;MyDatabricksJob&quot;, &quot;new_cluster&quot;: { &quot;spark_version&quot;: &quot;7.3.x-scala2.12&quot;, &quot;node_type_id&quot;: &quot;r3.xlarge&quot;, &quot;aws_attributes&quot;: { &quot;availability&quot;: &quot;ON_DEMAND&quot; }, &quot;num_workers&quot;: 10 }, &quot;spark_python_task&quot;: { &quot;python_file&quot;: &quot;/Users/ashton/databricks_task/main.py&quot;, &quot;parameters&quot;: python_params, }, } @task def get_run_now_config(notebook_params: dict): &quot;&quot;&quot; RunNow config template for the DatabricksSubmitRun task, Notebook Task params must be passed as a dictionary. &quot;&quot;&quot; return {&quot;job_id&quot;: 42, &quot;notebook_params&quot;: notebook_params} conn = PrefectSecret(&quot;DATABRICKS_CONNECTION_STRING&quot;) # Initialize Databricks task class as a template # We will then use the task function to pass in unique config options &amp; params RunNow = DatabricksRunNow(conn) SubmitRun = DatabricksSubmitRun(conn) with Flow(&quot;Databricks-Tasks&quot;, schedule=None) as flow: run_now_config = get_run_now_config({&quot;param1&quot;: &quot;value&quot;}) submit_config_a = get_submit_config([&quot;param1&quot;]) submit_config_b = get_submit_config([&quot;param2&quot;]) run_now_task = RunNow(json=run_now_config) submit_task_a = SubmitRun(json=submit_config_a) submit_task_b = SubmitRun(json=submit_config_b) # Since Databricks tasks don&#39;t return any data dependencies we can leverage, # we have to define the dependencies between Databricks tasks themselves flow.add_edge(run_now_task, submit_task_a) flow.add_edge(submit_task_a, submit_task_b) flow.run() # flow.register(&quot;YOUR_PROJECT&quot;) to register your flow on the UI . Conclusion . You now have all the knowledge you need to run Databricks Notebooks and Spark jobs as part of your ETL flows. For more information on Prefect and Databricks jobs, I recommend reading their documentation found here and here. . Feedback . As always, I encourage any feedback about my post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help. . You can also reach me and follow me on Twitter at @ashtonasidhu. . References . https://docs.prefect.io/core/, Prefect Documentation . | https://docs.prefect.io/core/getting_started/first-steps.html, Prefect Getting Started . |",
            "url": "https://ashton-sidhu.github.io/blog/prefect/databricks/etl/tutorial/guide/spark/2020/11/02/prefect-databricks.html",
            "relUrl": "/prefect/databricks/etl/tutorial/guide/spark/2020/11/02/prefect-databricks.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tutorial: Integrating Python with Compiler Languages",
            "content": "Python - a programming language that is human readable, easy to learn, and most importantly, easy to use. It is no wonder that it is one of the most popular languages today. From being able to build web applications, create automation scripts, analyze data and build machine learning models, Python is a jack of all trades. As with anything it has its short comings which include its speed and lack of granular access to machine hardware. This is due to Python being an interpreter-based language and not a compiler-based language. . Luckily for us, there is a way to solve that problem. Python has a native library that allows you to use functions built in compiled languages. This gives you the ability to leverage both the powers of Python and compiler based languages. Some of the most popular data science libraries such as Pandas and XGBoost have modules written in C/C++ (compiler-based languages) to overcome Python’s speed and performance issues while still having a user friendly Python API. . In this post, we are going to walk through interpreter based languages vs. compiler based languages, Python’s built in ctypes module and an example of how to use modules built from a compiled language. From there you will be able to integrate Python with other languages in your data science or machine learning projects. . Interpreters vs. Compilers . As I mentioned above, Python is an interpreted language. In order to execute code on a machine, the code first has to get translated to “machine code”. . . Machine code is just binary or hexadecimal instructions. The main difference between interpreted and compiled languages is that interpreted languages get executed line by line and passed through a translator that converts each line to machine code. . Compilers require a build step that translates all the code at once into an application (or binary) that can be executed. During the build phase there is a compiler that will optimize the translation from written code to machine code. The compiler’s optimizations are one of compiled based-languages are faster than interpreter-based languages. Common compiled languages are C, C++ and Go. . . This a high level comparison between interpreters and compilers. More in-depth knowledge would require individual research or a separate topic entirely. C Types . Python has built in libraries to be able to call modules built in compiled languages. It gives developers the ultimate flexibility to use compiled languages for tasks that Python isn’t well equipped to handle - all the while still building the core of the application in Python. . The main library that we will be using to interact with modules from a compiled application is ctypes. It provides C compatible data types and calling functions from applications built in compiled languages. It gives you the ability to wrap these languages in pure Python. To be able to do this, Python first has to convert its function argument types into C native types. This allows it to be compatible with compiled language function’s argument types. The figure below explains the process at a high level when interacting with functions from compiled languages. . . The example in the next section will be using Go so the image is Go specific, but the principle applies to the other compiled languages. . Walk Through . Below is a Go function I wrote that gets the closing price of a stock using the Alpha Vantage API. I’ll go over the key lines that allow us to create a Python wrapper for it. . package main import &quot;C&quot; import ( &quot;encoding/json&quot; &quot;fmt&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;net/http&quot; &quot;os&quot; &quot;time&quot; ) // APIKEY ... Alpha Vantage API key, stored as env variable var APIKEY string = os.Getenv(&quot;ALPHA_API_KEY&quot;) // ENDPOINT ... Alpha Vantage API daily stock data endpoint var ENDPOINT string = &quot;https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&quot; // Data ... Data structure type Data struct { MetaData MetaData `json:&quot;Meta Data&quot;` TimeSeries map[string]interface{} `json:&quot;Time Series (Daily)&quot;` } // MetaData ... Stock metadata structure type MetaData struct { Info string `json:&quot;1. Information&quot;` Symbol string `json:&quot;2. Symbol&quot;` LastRefresh string `json:&quot;3. Last Refreshed&quot;` OutputSize string `json:&quot;4. Output Size&quot;` TimeZone string `json:&quot;5. Time Zone&quot;` } // FinData ... Daily Financial Data json structure type FinData struct { Open string `json:&quot;1. open&quot;` High string `json:&quot;2. high&quot;` Low string `json:&quot;3. low&quot;` Close string `json:&quot;4. close&quot;` AdjClose string `json:&quot;5. adjusted close&quot;` Volume string `json:&quot;6. volume&quot;` DivAmount string `json:&quot;7. dividend amount&quot;` SplitCoeff string `json:&quot;8. split coefficient&quot;` } func main() {} //export getPrice func getPrice(ticker *C.char, date *C.char) *C.char { tickerDate := C.GoString(date) stock := C.GoString(ticker) query := fmt.Sprintf(&quot;%s&amp;symbol=%s&amp;apikey=%s&quot;, ENDPOINT, stock, APIKEY) client := http.Client{ Timeout: time.Second * 10, // Timeout after 5 seconds } req, err := http.NewRequest(http.MethodGet, query, nil) if err != nil { log.Fatal(err) } req.Header.Set(&quot;User-Agent&quot;, &quot;stock-api-project&quot;) resp, getErr := client.Do(req) if getErr != nil { log.Fatal(getErr) } defer resp.Body.Close() respBody, _ := ioutil.ReadAll(resp.Body) dailyData := Data{} json.Unmarshal(respBody, &amp;dailyData) // Encode Interface as bytes dailyFinDataMap := dailyData.TimeSeries[tickerDate] dfdByte, _ := json.Marshal(dailyFinDataMap) // Map interface to FinData struct dailyFinData := FinData{} json.Unmarshal(dfdByte, &amp;dailyFinData) return C.CString(dailyFinData.AdjClose) } . import “C” - Import the C package (aka cgo) to have access to C data types. . func main() {} - An empty main function to ensure we still have an executable. . //export getPrice - Export the function so that we can expose it to be accessed by Python. . func getPrice(ticker *C.char, date *C.char) *C.char { - The function arguments need to be C types. . tickerDate := C.GoString(date) - Convert the function arguments to their native Go types. . return C.CString(dailyFinData.AdjClose) - The return value has to be a native C type. . To be able to use this in Python we have to compile this program, go build -o stock-api.so -buildmode=c-shared main.go . Below is the associating Python wrapper that calls our exported Go function getPrice. . from ctypes import * def get_price(ticker: str) -&gt; Dict[str, str]: &quot;&quot;&quot;Gets the adjusted closing price of all stocks.&quot;&quot;&quot; lib = cdll.LoadLibrary(&quot;./stock-api.so&quot;) lib.getPrice.argtypes = [c_char_p, c_char_p] lib.getPrice.restype = c_char_p curr_date = str(date.today()) price = lib.getPrice(ticker.encode(), curr_date.encode()).decode() return price . from ctypes import * - Import everything from ctypes as per the documentation. . lib = cdll.LoadLibrary(&quot;./stock-api.so&quot;) - Load in the binary or compiled application. . lib.getPrice.argtypes = [c_char_p, c_char_p] - Set the getPrice function argument types to the corresponding C types. . lib.getPrice.restype = c_char_p - Set the return type to the corresponding C type. . price = lib.getPrice(ticker.encode(), curr_date.encode()).decode() - We call the getPrice function and convert the Python string arguments into bytes that can be passed to the Go function by calling .encode. We then receive the output from Go function as bytes so we decode it to convert it to a Python string. . Conclusion . You now have most of the knowledge you need to get started creating wrappers for compiled language functions. No longer are you bound by the cons of interpreter based languages. You can create modules in languages that better suit your use case, and then fall back on Python to build out a user friendly API or the core of your application. Happy coding! . Feedback . I encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com . .",
            "url": "https://ashton-sidhu.github.io/blog/python/data%20science/go/machine%20learning/2020/10/25/ctypes-go.html",
            "relUrl": "/python/data%20science/go/machine%20learning/2020/10/25/ctypes-go.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "A Data Scientist's Guide to Hacktoberfest",
            "content": "What is Hacktoberfest . . Open source projects are a pillar for the data industry, without them projects such as Pandas, Numpy Sci-kit Learn and the entire big data Apache stack would not exist. Hacktoberfest is a month long event where developers can contribute to open source projects on Github and receive free swag. To be eligible to receive the free swag, you must commit four successful Pull Requests on projects that have opted in to Hacktoberfest. To try and easily get the free swag, some contributors submit non meaningful contributions, leading to unhappy project maintainers. . In this post we’ll go through how you can give back to the community and make a meaningful contributions open source projects. . How I got into Open Source . . I first got into Open Source when I was working with two projects, interpret and pandas-summary. In both cases the usage of the projects had not completely covered my use cases, albeit they were very minor. . With interpret I was trying to use the library to explain/interpret a Sklearn classifier. The underlying issue is that the .predict method returns an integer (0, 1, etc.) for classifiers. When these integer values were used to calculate the Morris Sensitivity, an error was thrown saying that the data had to be a float. In this case, the fix was simple - cast the integers returned as integers by the Sklearn classifier to a float! Before the fix, anyone who tried to interpret a Sklearn classifier would get an error and this simple contribution allowed all users to interpret Sklearn classification models. . With pandas-summary, I was using this library to automate descriptive and statistical analysis of column data. The only issue was that every time I used the library, a histogram was always plotted, which was a nuisance. As a quality of life fix, I added a simple flag that users could specify if they want to plot the histogram. . My first two contributions were nothing special, and were quite simple, but from these two contributions I learned the basics of how to make a Pull Request to a remote project, follow contribution guidelines as well as interact &amp; communicate with project maintainers via Github, Slack, etc. . From there I went on to be a major contributor of the pandas-bokeh library, contributed bug fixes and UI improvements to Nteract and added major feature integrations to Prefect. . Tips . . Learn how to make a Pull Request . A pull request is a method of contributing your code into a project. To start off, find a project you want to contribute to and fork the project by clicking the fork button at the top right corner of the project page. This will create a copy of the project in its current state under your repositories. . . Once you have forked the project, navigate to the repository under your Projects in Github. Under the project name it should say forked from .... . Clone the forked project to your local computer. . Create a new branch for you change or bug fix: git checkout -b your_branch_name. . Make your changes, commit them following the Contribution Guidelines of the project. . Push your changes to your forked project repository: git push -u origin your_branch_name. . Navigate to the forked repository project page. You will see a prompt to create a pull request. If you don’t navigate to the pull requests tab and create one from there by selecting New Pull Request. . . Fill out the pull request template, if there is one, or one that is outlined in the Contributing guidelines. Once that is completed, your pull request is good to go and wait for feedback! . Start with a library you use often, big or small . The first step towards your first open source contribution is to pick a project you want to contribute to. The best way that I have experienced is to contribute to projects you use often or are actively using. If you have extensively used a project, you have probably come across some across some bugs or enhancements that will improve the quality of the project. This is how I started my journey in Open Source, by trying to improve projects that I was using daily. . Look at existing Github Issues . If you are looking for an issue or way to contribute to a project, a good place to start is Github’s built in Issues tab. This is where users and project maintainers can log bugs and feature enhancements. Project maintainers will go through these issues and tag them, gather more information, add meta data, etc. One of the tags they will add is a “good first issue” tag to inform potential contributors that this issue is good for first time contributors or contributors who are new to the project. . . These issues are recommended for contributors who are either new to open source or to the project itself to help them get started and make a contribution. Leverage these if you can’t find your own bug to fix or enhancement to add. . Read the Contribution Guidelines . There is nothing worse than putting in all this work of finding a project, isolating a bug or developing a new feature and to have it rejected or not even looked at because you didn’t follow coding, formatting or commit message standards. The good news is that all of these are available, usually in a Contribution section in the projects README or in the Contributing Guidelines section of the project. Projects will often have automated formatting checks that run when you create a pull request and your pull request won’t often be looked at until these basic checks are passed. . . If you don&#39;t see a contribution section or a project doesn&#39;t have Contributing Guidelines, don&#39;t just do whatever you want.&emsp;1) Follow the coding &amp; documentation style in the project as close as you can.&emsp;2) Follow coding &amp; documentation best practices. Every Contribution Matters . Every contribution in open source matters regardless how big or small. Whether it’s from a usability perspective or reading the documentation, if you are experiencing a bug or a grievance with a project there are others who are experiencing it as well. Documentation is a large component of open source software as every project needs it. It is often a great place to start contributing to a project as you gain an understanding of what a project is about. It provides background information of design decisions and considerations of the project, which will in turn help you in understand the code. . Documentation is the first place users go to find information and the more thoroughly documented a project is, the more of a user base it will have. Developers love documented projects and when a project doesn’t have documentation, or has poor documentation, a developer will think twice before adding it to their work flow. Adding documentation or fixing even the smallest bug may impact hundreds or even thousands of users who use that project every day and many will thank you for it. . What Happens Next . . Very rarely will your contribution get merged right away. Within a couple of days someone from the project team will comment their feedback or notify you that they are reviewing your pull request. Address the comments, ask questions, clarify anything you do not understand, make the proposed changes, if there are any, and your change with get merged shortly after! . If you do not receive any feedback on your pull request within a week, message a project maintainer and politely ask them what the status is. On larger projects there are often tons of pull requests and they may have forgotten about the pull request or have not got around to reviewing it yet. . If at this point you have not received a reply, which does not happen often (has never happened to me), take the skills and learning points from this project and move on to the next project. Once you make the pull request and have messaged the project maintainers, the rest is out of your hands. This is the only real unfortunate part of open source and one you should not take to heart. . Benefits . . Become a better Engineer or Scientist . Whether you are a Data Engineer, ML Engineer or Data Scientist contributing to Open Source will help you become better and progress in your field. From understanding how projects are built and structured, gaining deep intricate knowledge of a key library, navigating large code bases, writing production level code or even just learning a new method to solve a problem. All of these skills will translate directly into your profession or your next project. . Meet new Engineers, Developers &amp; Scientists . The greatest benefit of contributing to Open Source is the opportunity to work and interact with the minds who created a tool that is used by thousands of people world wide. You get direct insight into how they they created a solution to solve a widespread problem. Furthermore, you may end up connecting, bounce ideas off one another and collaborate on future projects together. Personally, I’ve connected with project maintainers whose project I have contributed to and have kept in touch with them on Twitter and LinkedIn. . Conclusion . . Today you may not be able to contribute a new feature, but being around the project, reading the code, reading the documentation, all of it gives you insight into the project. From there, a small contribution to documentation, might lead to a bug fix that was documented, which leads to a higher understanding of the project, which then leads to your first feature. . The moral of the story is, contribute in any way you can and eventually you will reach that goal of developing the new feature, starting your own OS project and even becoming a key contributor to a project. . Feedback . . I encourage any and all feedback about any of my posts. You can message me on twitter or e-mail me at sidhuashton@gmail.com. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/data%20science/open%20source/hacktoberfest/ml/2020/10/08/hacktoberfest-help.html",
            "relUrl": "/markdown/data%20science/open%20source/hacktoberfest/ml/2020/10/08/hacktoberfest-help.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
            "content": "Jupyter Notebook provides a great platform to produce human-readable documents containing code, equations, analysis, and their descriptions. Some even consider it a powerful development when combining it with NBDev. For such an integral tool, the out of the box start up is not the best. Each use requires starting the Jupyter web application from the command line and entering your token or password. The entire web application relies on that terminal window being open. Some might “daemonize” the process and then use nohup to detach it from their terminal, but that’s not the most elegant and maintainable solution. . Lucky for us, Jupyter has already come up with a solution to this problem by coming out with an extension of Jupyter Notebooks that runs as a sustainable web application and has built-in user authentication. To add a cherry on top, it can be managed and sustained through Docker allowing for isolated development environments. . By the end of this post we will leverage the power of JupyterHub to access a Jupyter Notebook instance which can be accessed without a terminal, from multiple devices within your network, and a more user friendly authentication method. . Prerequisites . . A basic knowledge of Docker and the command line would be beneficial in setting this up. . I recommend doing this on the most powerful device you have and one that is turned on for most of the day, preferably all day. One of the benefits of this setup is that you will be able to use Jupyter Notebook from any device on your network, but have all the computation happen on the device we configure. . What is Jupyter Hub . . JupyterHub brings the power of notebooks to groups of users. The idea behind JupyterHub was to scale out the use of Jupyter Notebooks to enterprises, classrooms, and large groups of users. Jupyter Notebook, however, is supposed to run as a local instance, on a single node, by a single developer. Unfortunately, there was no middle ground to have the usability and scalability of JupyterHub and the simplicity of running a local Jupyter Notebook. That is, until now. . JupyterHub has pre-built Docker images that we can utilize to spawn a single notebook on a whim, with little to no overhead in technical complexity. We are going to use the combination of Docker and JupyterHub to access Jupyter Notebooks from anytime, anywhere, at the same URL. . Architecture . . The architecture of our JupyterHub server will consist of 2 services: JupyterHub and JupyterLab. JupyterHub will be the entry point and will spawn JupyterLab instances for any user. Each of these services will exist as a Docker container on the host. . . Building the Docker Images . . To build our at-home JupyterHub server we will use the pre-built Docker images of JupyterHub &amp; JupyterLab. . Dockerfiles . The JupyterHub Docker image is simple. . FROM jupyterhub/jupyterhub:1.2 # Copy the JupyterHub configuration in the container COPY jupyterhub_config.py . # Download script to automatically stop idle single-user servers COPY cull_idle_servers.py . # Install dependencies (for advanced authentication and spawning) RUN pip install dockerspawner . We use the pre-built JupyterHub Docker Image and add our own configuration file to stop idle servers, cull_idle_servers.py. Lastly, we install additional packages to spawn JupyterLab instances via Docker. . Docker Compose . To bring everything together, let’s create a docker-compose.yml file to define our deployments and configuration. . version: &#39;3&#39; services: # Configuration for Hub+Proxy jupyterhub: build: . # Build the container from this folder. container_name: jupyterhub_hub # The service will use this container name. volumes: # Give access to Docker socket. - /var/run/docker.sock:/var/run/docker.sock - jupyterhub_data:/srv/jupyterlab environment: # Env variables passed to the Hub process. DOCKER_JUPYTER_IMAGE: jupyter/tensorflow-notebook DOCKER_NETWORK_NAME: ${COMPOSE_PROJECT_NAME}_default HUB_IP: jupyterhub_hub ports: - 8000:8000 restart: unless-stopped # Configuration for the single-user servers jupyterlab: image: jupyter/tensorflow-notebook command: echo volumes: jupyterhub_data: . The key environment variables to note are DOCKER_JUPYTER_IMAGE and DOCKER_NETWORK_NAME. JupyterHub will create Jupyter Notebooks with the images defined in the environment variable.For more information on selecting Jupyter images you can visit the following Jupyter documentation. . DOCKER_NETWORK_NAME is the name of the Docker network used by the services. This network gets an automatic name from Docker Compose, but the Hub needs to know this name to connect the Jupyter Notebook servers to it. To control the network name we use a little hack: we pass an environment variable COMPOSE_PROJECT_NAME to Docker Compose, and the network name is obtained by appending _default to it. . Create a file called .env in the same directory as the docker-compose.yml file and add the following contents: . COMPOSE_PROJECT_NAME=jupyter_hub . Stopping Idle Servers . Since this is our home setup, we want to be able to stop idle instances to preserve memory on our machine. JupyterHub has services that can run along side it and one of them being jupyterhub-idle-culler. This service stops any instances that are idle for a prolonged duration. . To add this servive, create a new file called cull_idle_servers.py and copy the contents of jupyterhub-idle-culler project into it. . . Ensure `cull_idle_servers.py` is in the same folder as the Dockerfile. To find out more about JupyterHub services, check out their official documentation on them. . Jupyterhub Config . To finish off, we need to define configuration options such, volume mounts, Docker images, services, authentication, etc. for our JupyterHub instance. . Below is a simple jupyterhub_config.py configuration file I use. . import os import sys c.JupyterHub.spawner_class = &#39;dockerspawner.DockerSpawner&#39; c.DockerSpawner.image = os.environ[&#39;DOCKER_JUPYTER_IMAGE&#39;] c.DockerSpawner.network_name = os.environ[&#39;DOCKER_NETWORK_NAME&#39;] c.JupyterHub.hub_connect_ip = os.environ[&#39;HUB_IP&#39;] c.JupyterHub.hub_ip = &quot;0.0.0.0&quot; # Makes it accessible from anywhere on your network c.JupyterHub.admin_access = True c.JupyterHub.services = [ { &#39;name&#39;: &#39;cull_idle&#39;, &#39;admin&#39;: True, &#39;command&#39;: [sys.executable, &#39;cull_idle_servers.py&#39;, &#39;--timeout=42000&#39;] }, ] c.Spawner.default_url = &#39;/lab&#39; notebook_dir = os.environ.get(&#39;DOCKER_NOTEBOOK_DIR&#39;) or &#39;/home/jovyan/work&#39; c.DockerSpawner.notebook_dir = notebook_dir c.DockerSpawner.volumes = { &#39;/home/sidhu&#39;: &#39;/home/jovyan/work&#39; } . Take note of the following configuration options: . &#39;command&#39;: [sys.executable, &#39;cull_idle_servers.py&#39;, &#39;--timeout=42000&#39;] : Timeout is the number of seconds until an idle Jupyter instance is shut down. . | c.Spawner.default_url = &#39;/lab&#39;: Uses Jupyterlab instead of Jupyter Notebook. Comment out this line to use Jupyter Notebook. . | &#39;/home/sidhu&#39;: &#39;/home/jovyan/work&#39;: I mounted my home directory to the JupyterLab home directory to have access to any projects and notebooks I have on my Desktop. This also allows us to achieve persistence in the case we create new notebooks, they are saved to our local machine and will not get deleted if our Jupyter Notebook Docker container is deleted. . | . Remove this line if you do not wish to mount your home directory and do not forget to change sidhu to your user name. . Start the Server . . To start the server, simply run docker-compose up -d, navigate to localhost:8000 in your browser and you should be able to see the JupyterHub landing page. . . To access it on other devices on your network such asva laptop, an iPad, etc, identify the IP of the host machine by running ifconfig on Unix machines &amp; ipconfig on Windows. . . From your other device, navigate to the IP you found on port 8000: http://IP:8000 and you should see the JupyterHub landing page! . Authenticating . That leaves us with the last task of authenticating to the server. Since we did not set up a LDAP server or OAuth, JupyterHub will use PAM (Pluggable Authentication Module) authentication to authenticate users. This means JupyterHub uses the user name and passwords of the host machine to authenticate. . To make use of this, we will have to create a user on the JupyterHub Docker container. There are other ways of doing this such as having a script placed on the container and executed at container start up but we will do it manually as an exercise. If you tear down or rebuild the container you will have to recreate users. . . I do not recommend hard coding user credentials into any script or Dockerfile. 1) Find the JupyterLab container ID: docker ps -a . . 2) “SSH” into the container: docker exec -it $YOUR_CONTAINER_ID bash . 3) Create a user and follow the terminal prompts to create a password: useradd $YOUR_USERNAME . 4) Sign in with the credentials and you’re all set! . . You now have a ready to go Jupyter Notebook server that can be accessed from any device, in the palm of your hands! Happy Coding! . Feedback . . I encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/data%20science/jupyter/ml/2020/10/03/jupyterhub.html",
            "relUrl": "/markdown/data%20science/jupyter/ml/2020/10/03/jupyterhub.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "What's New in Aethos 2.0",
            "content": "In late 2019 I released Aethos 1.0, the first iteration of a package to automate common data science techniques. Since then I’ve received great feedback on how to improve Aethos which I’ll introduce here! It will be a lot of code examples to show the power and versatility of the package. . You can view the previous posts about Aethos on my blog! . Intro to Aethos . Modelling with Aethos . What is Aethos? . For those new to Aethos, Aethos is a Python library of automated data science techniques and use cases from missing value imputation, NLP pre-processing, feature engineering, data visualization to modelling, model analysis and model deployment. . To see the full capabilities and the rest of the techniques and models you can run, checkout the project page on Github! . Problems with Aethos 1.0 . Alot of the problems with the first version of Aethos were related to the usability of the package and its API. The major problems were: . Slow import times due to the number of files and coupled packages. . | Having 2 objects for end to end analysis - Data for transformations and Model for modelling . | Model object had every model and was not specific to Supervised or Unsupervised problems. . | Unintuitive API calls for adding new columns to the underlying DataFrames . | Reporting feature was, well, garbage and becoming redundant with external tools like converting notebooks to pdfs. . | API had limited use cases. You couldn&#39;t just analyze your data, or just analyze a model you trained without Aethos. . | Aethos and Pandas were not interchangeable and did not work together when transforming data. . | . What&#39;s new in Aethos 2.0 . Aethos 2.0 looks to address the intuitiveness and usability of the package to make it easier to use and understand. It also addresses the ability to work with Pandas Dataframes side by side with Aethos. . Reduced import time of the package by simplifying and decoupling of the Aethos modules. . | Only 1 object to analyze, visualize, transform, model and analyze results. . | Can now specify the type of problem - Classification, Regression or Unsupervised and only see the models specific to those problems. . | Removed the complexity of adding data to the underlying dataframes through Aethos objects. You can access the underlying dataframes with the x_train and x_test properties. . | Removed reporting feature. . | Introduced new objects to support new cases: . Analysis: To analyze, visualize and run statistical analysis (t-test, anova, etc.) on your data. . | Classification: To analyze, visualize, run statistical analysis, transform and impute your data to run classification models. . | Regression: To analyze, visualize, run statistical analysis, transform and impute your data to run regression models. . | Unsupervised: To analyze, visualize, run statistical analysis, transform and impute your data to run unsupervised models. . | ClassificationModelAnalysis: Interpret, analyze and visualize classification model results. . | RegressionModelAnalysis: Interpret, analyze and visualize regression model results. . | UnsupervisedModelAnalysis: Interpret, analyze and visualize unsupervised model results. . | TextModelAnalysis: Interpret, analyze and visualize text model results. . | . | Removed dot notation when accessing DataFrame columns. . | Can now chain methods together. . | . . Note: The model analysis objects get automatically initialized when you run a model with Aethos. They can also be initialized by themselves by supplying a model object, train data and test data. . Examples . !pip install aethos . import pandas as pd import aethos as at at.options.track_experiments = True # Enable experiment tracking with MLFlow . To showcase each of the objects let&#39;s load in the titanic dataset. . orig_data = pd.read_csv(&#39;https://raw.githubusercontent.com/Ashton-Sidhu/aethos/develop/examples/data/train.csv&#39;) . orig_data.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . Analysis . The analysis objects is mainly for quick, easy analysis and visualization of data. It doesn&#39;t have the ability to run automated cleaning and transformation techniques of Aethos, just visualizations and statistical tests. It also does not split your data, but you do have the option to provide a test set. . df = at.Analysis(orig_data, target=&#39;Survived&#39;) . df.describe() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . count 891 | 891 | 891 | NaN | NaN | 714 | 891 | 891 | NaN | 891 | NaN | NaN | . mean 446 | 0.383838 | 2.30864 | NaN | NaN | 29.6991 | 0.523008 | 0.381594 | NaN | 32.2042 | NaN | NaN | . std 257.354 | 0.486592 | 0.836071 | NaN | NaN | 14.5265 | 1.10274 | 0.806057 | NaN | 49.6934 | NaN | NaN | . min 1 | 0 | 1 | NaN | NaN | 0.42 | 0 | 0 | NaN | 0 | NaN | NaN | . 25% 223.5 | 0 | 2 | NaN | NaN | 20.125 | 0 | 0 | NaN | 7.9104 | NaN | NaN | . 50% 446 | 0 | 3 | NaN | NaN | 28 | 0 | 0 | NaN | 14.4542 | NaN | NaN | . 75% 668.5 | 1 | 3 | NaN | NaN | 38 | 1 | 0 | NaN | 31 | NaN | NaN | . max 891 | 1 | 3 | NaN | NaN | 80 | 8 | 6 | NaN | 512.329 | NaN | NaN | . counts 891 | 891 | 891 | 891 | 891 | 714 | 891 | 891 | 891 | 891 | 204 | 889 | . uniques 891 | 2 | 3 | 891 | 2 | 88 | 7 | 7 | 681 | 248 | 147 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 177 | 0 | 0 | 0 | 0 | 687 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 19.87% | 0% | 0% | 0% | 0% | 77.10% | 0.22% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.missing_values . Train set missing values. Total Percent . Cabin 687 | 77.10% | . Age 177 | 19.87% | . Embarked 2 | 0.22% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | . df.column_info() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . counts 891 | 891 | 891 | 891 | 891 | 714 | 891 | 891 | 891 | 891 | 204 | 889 | . uniques 891 | 2 | 3 | 891 | 2 | 88 | 7 | 7 | 681 | 248 | 147 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 177 | 0 | 0 | 0 | 0 | 687 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 19.87% | 0% | 0% | 0% | 0% | 77.10% | 0.22% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.standardize_column_names() . passengerid pclass name sex age sibsp parch ticket fare cabin embarked survived . 0 1 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 1 | . 2 3 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 1 | . 3 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 1 | . 4 5 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . df.describe_column(&#39;fare&#39;) . mean 32.2042 std 49.6934 variance 2469.44 min 0 max 512.329 mode 8.05 5% 7.225 25% 7.9104 50% 14.4542 75% 31 95% 112.079 iqr 23.0896 kurtosis 33.3981 skewness 4.78732 sum 28693.9 mad 28.1637 cv 1.54307 zeros_num 15 zeros_perc 1.68% deviating_of_mean 20 deviating_of_mean_perc 2.24% deviating_of_median 53 deviating_of_median_perc 5.95% top_correlations counts 891 uniques 248 missing 0 missing_perc 0% types numeric Name: fare, dtype: object . df.data_report() . Summarize dataset: 100%|██████████| 26/26 [00:04&lt;00:00, 6.31it/s, Completed] Generate report structure: 100%|██████████| 1/1 [00:01&lt;00:00, 1.94s/it] Render HTML: 100%|██████████| 1/1 [00:00&lt;00:00, 1.07it/s] . . Easily view the histogram of multiple features. . df.histogram(&#39;age&#39;, &#39;fare&#39;, hue=&#39;survived&#39;) . Create a configurable correlation matrix. . df.correlation_matrix(data_labels=True, hide_mirror=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd038322748&gt; . We can easily plot the average price each age paid for a ticket. . df.barplot(x=&#39;age&#39;, y=&#39;fare&#39;, method=&#39;mean&#39;, labels={&#39;age&#39;: &#39;Age&#39;, &#39;fare&#39;: &#39;Fare&#39;}, asc=False) . . . We can also easily view the relationship between age and fair and see the difference between those who survived and who didn&#39;t. . df.scatterplot(x=&#39;age&#39;, y=&#39;fare&#39;, color=&#39;survived&#39;, labels={&#39;age&#39;: &#39;Age&#39;, &#39;fare&#39;: &#39;Fare&#39;}, marginal_x=&#39;histogram&#39;, marginal_y=&#39;histogram&#39;) . . . You can visualize other plots like raincloud, violin, box, pairwise, etc. I recommend checking out the examples for more! . One of the big changes is that ability to work with pandas side by side. If you want to transform and work with data solely with Pandas, the Analysis object will reflect those changes. This allows you to use Aethos solely for automated analysis and Pandas for transformations. . To demonstrate this we will make a new boolean feature to see if a passenger was a child using the original pandas dataframe we created . orig_data[&#39;is_child&#39;] = (orig_data[&#39;age&#39;] &lt; 18).astype(int) orig_data.head() . passengerid survived pclass name sex age sibsp parch ticket fare cabin embarked is_child . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . Now let&#39;s see it in our Analysis object. . df.head() . passengerid survived pclass name sex age sibsp parch ticket fare cabin embarked is_child . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | 0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | 0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | 0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | 0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | 0 | . df.boxplot(x=&#39;is_child&#39;, y=&#39;fare&#39;, color=&#39;survived&#39;) . . . You can still run pandas functions on Aethos objects. . df.nunique() . passengerid 891 survived 2 pclass 3 name 891 sex 2 age 88 sibsp 7 parch 7 ticket 681 fare 248 cabin 147 embarked 3 is_child 2 dtype: int64 . df[&#39;age&#39;].nunique() . 88 . New Features . Introduced in Aethos 2.0 are some new analytic techniques. . Predictive Power Score . The predictive power score is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). Credits go to 8080Labs for creating this library and you can get more info here . df.predictive_power(data_labels=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd0384a2198&gt; . AutoViz . AutoViz auto visualizes your data and displays key plots based off the characteristics of your data. Credits go to AutoViML for creating this library and you can get more info here. . df.autoviz() . Imported AutoViz_Class version: 0.0.68. Call using: from autoviz.AutoViz_Class import AutoViz_Class AV = AutoViz_Class() AutoViz(filename, sep=&#39;,&#39;, depVar=&#39;&#39;, dfte=None, header=0, verbose=0, lowess=False,chart_format=&#39;svg&#39;,max_rows_analyzed=150000,max_cols_analyzed=30) To remove previous versions, perform &#39;pip uninstall autoviz&#39; Shape of your Data Set: (891, 13) Classifying variables in data set... 12 Predictors classified... This does not include the Target column(s) 4 variables removed since they were ID or low-information variables Total Number of Scatter Plots = 3 Nothing to add Plot not being added All plots done Time to run AutoViz (in seconds) = 2.659 . Modelling . Aethos 2.0 introduces 3 new model objects: Classification, Regression and Unsupervised. These objects have the same capabilities of the Analysis object, but also can transform your data the same way it did in Aethos 1.0. For those new to Aethos, whenever you use Aethos to apply a transformation, it fits it to the training data and applies it to both the training and test data (in the case of Classification and Regression) to avoid data leakage. . In this post we&#39;ll cover the Classification object but the process is the exact same if you were working with a Regression or Unsupervised problem. . df = at.Classification(orig_data, target=&#39;Survived&#39;, test_split_percentage=.25) . As with Aethos 1.0 if no test data is provided, it is split upon initialization. In Aethos 2.0 it uses stratification for classification problems to split the data to ensure some resemblance of class balance. . . Warning: Earlier we showed the ability to alter the original dataframe and have it reflected in the Aethos object. This is NOT the case if you do not provide a test set for the Classification and Regression object. . df.describe() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . count 668 | 668 | 668 | NaN | NaN | 533 | 668 | 668 | NaN | 668 | NaN | NaN | . mean 441.913 | 0.383234 | 2.29192 | NaN | NaN | 29.4192 | 0.510479 | 0.377246 | NaN | 32.4659 | NaN | NaN | . std 260.048 | 0.486539 | 0.841285 | NaN | NaN | 14.7713 | 1.08757 | 0.781087 | NaN | 51.5116 | NaN | NaN | . min 1 | 0 | 1 | NaN | NaN | 0.42 | 0 | 0 | NaN | 0 | NaN | NaN | . 25% 214.75 | 0 | 1.75 | NaN | NaN | 20 | 0 | 0 | NaN | 7.925 | NaN | NaN | . 50% 450.5 | 0 | 3 | NaN | NaN | 28 | 0 | 0 | NaN | 14.4542 | NaN | NaN | . 75% 668.25 | 1 | 3 | NaN | NaN | 38 | 1 | 0 | NaN | 31.275 | NaN | NaN | . max 891 | 1 | 3 | NaN | NaN | 80 | 8 | 5 | NaN | 512.329 | NaN | NaN | . counts 668 | 668 | 668 | 668 | 668 | 533 | 668 | 668 | 668 | 668 | 160 | 666 | . uniques 668 | 2 | 3 | 668 | 2 | 82 | 7 | 6 | 545 | 216 | 121 | 3 | . missing 0 | 0 | 0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 508 | 2 | . missing_perc 0% | 0% | 0% | 0% | 0% | 20.21% | 0% | 0% | 0% | 0% | 76.05% | 0.30% | . types numeric | bool | numeric | unique | bool | numeric | numeric | numeric | categorical | numeric | categorical | categorical | . df.x_train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 482 | 0 | 2 | Frost, Mr. Anthony Wood &quot;Archie&quot; | male | NaN | 0 | 0 | 239854 | 0.0000 | NaN | S | . 1 828 | 1 | 2 | Mallet, Master. Andre | male | 1.0 | 0 | 2 | S.C./PARIS 2079 | 37.0042 | NaN | C | . 2 562 | 0 | 3 | Sivic, Mr. Husein | male | 40.0 | 0 | 0 | 349251 | 7.8958 | NaN | S | . 3 865 | 0 | 2 | Gill, Mr. John William | male | 24.0 | 0 | 0 | 233866 | 13.0000 | NaN | S | . 4 283 | 0 | 3 | de Pelsmaeker, Mr. Alfons | male | 16.0 | 0 | 0 | 345778 | 9.5000 | NaN | S | . df.x_test.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 187 | 1 | 3 | O&#39;Brien, Mrs. Thomas (Johanna &quot;Hannah&quot; Godfrey) | female | NaN | 1 | 0 | 370365 | 15.5000 | NaN | Q | . 1 321 | 0 | 3 | Dennis, Mr. Samuel | male | 22.0 | 0 | 0 | A/5 21172 | 7.2500 | NaN | S | . 2 379 | 0 | 3 | Betros, Mr. Tannous | male | 20.0 | 0 | 0 | 2648 | 4.0125 | NaN | C | . 3 698 | 1 | 3 | Mullens, Miss. Katherine &quot;Katie&quot; | female | NaN | 0 | 0 | 35852 | 7.7333 | NaN | Q | . 4 509 | 0 | 3 | Olsen, Mr. Henry Margido | male | 28.0 | 0 | 0 | C 4001 | 22.5250 | NaN | S | . df.missing_values . Train set missing values. Total Percent . Cabin 508 | 76.05% | . Age 135 | 20.21% | . Embarked 2 | 0.30% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | Test set missing values. Total Percent . Cabin 179 | 80.27% | . Age 42 | 18.83% | . Embarked 0 | 0.00% | . Fare 0 | 0.00% | . Ticket 0 | 0.00% | . Parch 0 | 0.00% | . SibSp 0 | 0.00% | . Sex 0 | 0.00% | . Name 0 | 0.00% | . Pclass 0 | 0.00% | . Survived 0 | 0.00% | . PassengerId 0 | 0.00% | . | . . Tip: Aethos comes with a checklist to help give you reminders when cleaning, analyzing and transforming your data! . df.checklist() . df.standardize_column_names() . passengerid pclass name sex age sibsp parch ticket fare cabin embarked survived . 0 482 | 2 | Frost, Mr. Anthony Wood &quot;Archie&quot; | male | NaN | 0 | 0 | 239854 | 0.0000 | NaN | S | 0 | . 1 828 | 2 | Mallet, Master. Andre | male | 1.0 | 0 | 2 | S.C./PARIS 2079 | 37.0042 | NaN | C | 1 | . 2 562 | 3 | Sivic, Mr. Husein | male | 40.0 | 0 | 0 | 349251 | 7.8958 | NaN | S | 0 | . 3 865 | 2 | Gill, Mr. John William | male | 24.0 | 0 | 0 | 233866 | 13.0000 | NaN | S | 0 | . 4 283 | 3 | de Pelsmaeker, Mr. Alfons | male | 16.0 | 0 | 0 | 345778 | 9.5000 | NaN | S | 0 | . Since this is an overview, let&#39;s select the columns were going to work with and drop the ones we&#39;re not going to use. . df.drop(keep=[&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;fare&#39;, &#39;embarked&#39;]) . pclass sex age fare embarked survived . 0 2 | male | NaN | 0.0000 | S | 0 | . 1 2 | male | 1.0 | 37.0042 | C | 1 | . 2 3 | male | 40.0 | 7.8958 | S | 0 | . 3 2 | male | 24.0 | 13.0000 | S | 0 | . 4 3 | male | 16.0 | 9.5000 | S | 0 | . Let&#39;s chain our transformations together. Remember our transformations will be fit to the training data and automatically transform our test data! . is_child = lambda df: 1 if df[&#39;age&#39;] &lt; 18 else 0 df.replace_missing_median(&#39;age&#39;) .replace_missing_mostcommon(&#39;embarked&#39;) .onehot_encode(&#39;sex&#39;, &#39;pclass&#39;, &#39;embarked&#39;, keep_col=False) .apply(is_child, &#39;is_child&#39;) .normalize_numeric(&#39;fare&#39;, &#39;age&#39;) . Pandas Apply: 100%|██████████| 668/668 [00:00&lt;00:00, 77148.31it/s] Pandas Apply: 100%|██████████| 223/223 [00:00&lt;00:00, 57466.81it/s] . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.000000 | 0.346569 | 0 | . 1 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1 | 0.072227 | 0.007288 | 1 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.015412 | 0.497361 | 0 | . 3 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.025374 | 0.296306 | 0 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1 | 0.018543 | 0.195778 | 0 | . df.x_train.head() . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.000000 | 0.346569 | 0 | . 1 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1 | 0.072227 | 0.007288 | 1 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.015412 | 0.497361 | 0 | . 3 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0 | 0.025374 | 0.296306 | 0 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1 | 0.018543 | 0.195778 | 0 | . df.x_test.head() . sex_female sex_male pclass_1 pclass_2 pclass_3 embarked_C embarked_Q embarked_S is_child fare age survived . 0 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0 | 0.030254 | 0.346569 | 1 | . 1 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.014151 | 0.271174 | 0 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0 | 0.007832 | 0.246042 | 0 | . 3 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0 | 0.015094 | 0.346569 | 1 | . 4 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0 | 0.043966 | 0.346569 | 0 | . Now let&#39;s train a Logistic Regression model. . We&#39;ll use gridsearch and it will automatically return the best model. We&#39;ll use Stratified K-fold for the Cross Validation technique during grid search. . gs_params = { &quot;C&quot;: [0.1, 0.5, 1], &quot;max_iter&quot;: [100, 1000] } lr = df.LogisticRegression( cv_type=&#39;strat-kfold&#39;, gridsearch=gs_params, random_state=42 ) . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . Gridsearching with the following parameters: {&#39;C&#39;: [0.1, 0.5, 1], &#39;max_iter&#39;: [100, 1000]} Fitting 5 folds for each of 6 candidates, totalling 30 fits . [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 0.2s finished . LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=42, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . Once a model is trained a ModelAnalysis object is returned which allows us to analyze, interpret and visualize our model results. Included is a list to help you debug your model if it’s overfit or underfit! . df.help_debug() . You can quickly cross validate any model by calling cross_validate on the resulting ModelAnalysis object. It will display the mean score across all folds and a learning curve. . For classification problems the default cross validation method is Stratified K-Fold. This allows to maintain some form of class balance, while for regression, the default is K-Fold. . lr.cross_validate() . lr.metrics() # Note this displays the results on the test data. . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . Balanced Accuracy 0.774 | The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class. | . Average Precision 0.822 | Summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Zero One Loss 0.220 | Fraction of misclassifications. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . Recall 0.744 | It measures how many observations out of all positive observations have we classified as positive. Good to use when catching call positive occurences, usually at the cost of false positive. | . Matthews Correlation Coefficient 0.542 | It’s a correlation between predicted classes and ground truth. | . Log Loss 0.450 | Difference between ground truth and predicted score for every observation and average those errors over all observations. | . Jaccard 0.566 | Defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of true labels. | . Hinge Loss 0.511 | Computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. | . Hamming Loss 0.220 | The Hamming loss is the fraction of labels that are incorrectly predicted. | . F-Beta 0.711 | It’s the harmonic mean between precision and recall, with an emphasis on one or the other. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . F1 0.723 | It’s the harmonic mean between precision and recall. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . Cohen Kappa 0.541 | Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies. Works well for imbalanced problems. | . Brier Loss 0.220 | It is a measure of how far your predictions lie from the true values. Basically, it is a mean square error in the probability space. | . Manual vs Automated . Lets&#39;s manually train a Logistic Regression and view and verify the results. . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score, precision_score X_train = df.x_train.drop(&quot;survived&quot;, axis=1) X_test = df.x_test.drop(&quot;survived&quot;, axis=1) y_train = df.x_train[&quot;survived&quot;] y_test = df.x_test[&quot;survived&quot;] clf = LogisticRegression(C=1, max_iter=100, random_state=42).fit(X_train, y_train) y_pred = clf.predict(X_test) print(f&quot;Accuracy: {accuracy_score(y_test, y_pred).round(3)}&quot;) print(f&quot;AUC: {roc_auc_score(y_test, clf.decision_function(X_test)).round(3)}&quot;) print(f&quot;Precision: {precision_score(y_test, y_pred).round(3)}&quot;) . Accuracy: 0.78 AUC: 0.853 Precision: 0.703 . Results are the same! . Model Analysis . Similar to Modelling, Aethos 2.0 introduces 4 model analysis objects: ClassificationModelAnalysis, RegressionModelAnalysis, UnsupervisedModelAnalysis and TextModelAnalysis. In Aethos 2.0 they can be initialized in 2 ways: . Result of training a model using Aethos . | Initializing it on your own by providing a Model object, the train data used by the model and the test data to evaluate model performance (for Regression and Classification). . | . Similar to the Model objects we&#39;re going to explore the ClassificationModelAnalysis object but the process would be the same for regression, unsupervised and text model analysis. . Initialzed from Aethos . To start, we&#39;ll pick off from where we left off with modelling and view the metrics for our Logistic Regression model. . type(lr) . aethos.model_analysis.classification_model_analysis.ClassificationModelAnalysis . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . Balanced Accuracy 0.774 | The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class. | . Average Precision 0.822 | Summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Zero One Loss 0.220 | Fraction of misclassifications. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . Recall 0.744 | It measures how many observations out of all positive observations have we classified as positive. Good to use when catching call positive occurences, usually at the cost of false positive. | . Matthews Correlation Coefficient 0.542 | It’s a correlation between predicted classes and ground truth. | . Log Loss 0.450 | Difference between ground truth and predicted score for every observation and average those errors over all observations. | . Jaccard 0.566 | Defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of true labels. | . Hinge Loss 0.511 | Computes the average distance between the model and the data using hinge loss, a one-sided metric that considers only prediction errors. | . Hamming Loss 0.220 | The Hamming loss is the fraction of labels that are incorrectly predicted. | . F-Beta 0.711 | It’s the harmonic mean between precision and recall, with an emphasis on one or the other. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . F1 0.723 | It’s the harmonic mean between precision and recall. Takes into account both metrics, good for imbalanced problems (spam, fraud, etc.). | . Cohen Kappa 0.541 | Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies. Works well for imbalanced problems. | . Brier Loss 0.220 | It is a measure of how far your predictions lie from the true values. Basically, it is a mean square error in the probability space. | . You can also set project metrics based off your business requirements. . at.options.project_metrics = [&quot;Accuracy&quot;, &quot;ROC AUC&quot;, &quot;Precision&quot;] . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . If you want to just view individual metrics, there are functions for those to! . lr.fbeta(beta=0.4999) . 0.7111085827756309 . You can analyze any models results with just one line of code: . Metrics | Classification Report | Confusion Matrix | Decision Boundaries | Decision Plots | Dependence Plots | Force Plots | LIME Plots | Morris Sensitivity | Model Weights | Summary Plot | RoC Curve | Individual metrics | . And this is only for Classification Models, each type of problem has their own set of ModelAnalysis functions! . lr.classification_report() . precision recall f1-score support 0 0.83 0.80 0.82 137 1 0.70 0.74 0.72 86 accuracy 0.78 223 macro avg 0.77 0.77 0.77 223 weighted avg 0.78 0.78 0.78 223 . lr.confusion_matrix() . You can supply features from your train set to the dependency plot otherwise it will just use the first 2 features in your model. Under the hood it uses YellowBricks Decision Boundary visualizer to create the visualizations. . lr.decision_boundary(&#39;age&#39;, &#39;fare&#39;) . lr.decision_boundary() . Included are also automated SHAP use cases to interpret your model! . lr.decision_plot() . &lt;shap.plots.decision.DecisionPlotResult at 0x7f521b43abd0&gt; . lr.dependence_plot(&#39;age&#39;) . lr.force_plot() . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. lr.interpret_model() . 100%|██████████| 111/111 [00:00&lt;00:00, 617.89it/s] . Open in new window&lt;iframe src=&quot;http://127.0.0.1:7664/139990620277328/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; View the highest weighted features in your model. . lr.model_weights() . age : -1.64 sex_male : -1.23 sex_female : 1.23 pclass_3 : -1.06 pclass_1 : 1.05 is_child : 0.56 fare : 0.46 embarked_S : -0.33 embarked_C : 0.20 embarked_Q : 0.13 pclass_2 : 0.00 . Easily plot an RoC curve. . lr.roc_curve() . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f521b2e1f90&gt; . lr.summary_plot() . Finally we can generate the files to deploy our model through a RESTful API using FastAPI, Gunicorn and Docker! . lr.to_service(&#39;aethos2&#39;) . Deployment files can be found at /home/sidhu/.aethos/projects/aethos2. To run: docker build -t `image_name` ./ docker run -d --name `container_name` -p `port_num`:80 `image_name` . User Initialization . If we manually trained a model like we did earlier in the notebook and wanted to use Aethos&#39;s model analysis capabilties we can! . lr = at.ClassificationModelAnalysis( clf, df.x_train, df.x_test, target=&#39;survived&#39;, model_name=&#39;log_reg&#39; ) . . Note: x_train and x_test datasets must have the target variable as part of the DataFrame. . You will receive the same results as above, thus giving you the ability to manually transform your data, train your model and use Aethos to interpret the results. I&#39;ve included them below for verification. . lr.metrics() . log_reg Description . Accuracy 0.780 | Measures how many observations, both positive and negative, were correctly classified. | . ROC AUC 0.853 | Shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. | . Precision 0.703 | It measures how many observations predicted as positive are positive. Good to use when False Positives are costly. | . lr.decision_boundary(&#39;age&#39;, &#39;fare&#39;) . lr.decision_boundary() . lr.decision_plot() . &lt;shap.plots.decision.DecisionPlotResult at 0x7f521cb2ed10&gt; . lr.dependence_plot(&#39;age&#39;) . lr.force_plot() . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. lr.interpret_model() . 100%|██████████| 111/111 [00:00&lt;00:00, 727.94it/s] . Open in new window&lt;iframe src=&quot;http://127.0.0.1:7664/139990657407168/&quot; width=100% height=800 frameBorder=&quot;0&quot;&gt;&lt;/iframe&gt; lr.model_weights() . age : -1.64 sex_male : -1.23 sex_female : 1.23 pclass_3 : -1.06 pclass_1 : 1.05 is_child : 0.56 fare : 0.46 embarked_S : -0.33 embarked_C : 0.20 embarked_Q : 0.13 pclass_2 : 0.00 . lr.roc_curve() . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f521d910c90&gt; . lr.summary_plot()b . lr.to_service(&#39;aethos2&#39;) . Deployment files can be found at /home/sidhu/.aethos/projects/aethos2. To run: docker build -t `image_name` ./ docker run -d --name `container_name` -p `port_num`:80 `image_name` . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/jupyter/aethos/datascience/2020/05/12/Aethos-2.0.html",
            "relUrl": "/jupyter/aethos/datascience/2020/05/12/Aethos-2.0.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
            "content": "Working on your own data science projects are a great opportunity to learn some new skills and hone existing skills, but what if you want to use technologies that you would use in industry such as Hadoop, Spark on a distributed cluster, Hive, etc. and have them all integrated? This is where the value comes from when building your own infrastructure. . You become familiar with the technologies, get to know the ins and outs about how it operates, debug and experience the different types of error messages and really get a sense of how the technology works over all instead of just interfacing with it. If you are also working with your own private data or confidential data in general, you may not want to upload it to an external service to do big data processing for privacy or security reasons. So, in this tutorial I’m going to walk through how to setup your own Big Data infrastructure on your own computer, home lab, etc. We’re going to setup a single node Hadoop &amp; Hive instance and a “distributed” spark cluster integrated with Jupyter. . Edit: Thanks to @Daniel Villanueva you can now deploy a VM with Hadoop, Spark and Hive pre-configured and ready to go through his Vagrant image. You can check it out on his Github here. . . This tutorial is not for an industry production installation! Prerequisites . A Debian based distro - Ubuntu, Pop-os, etc | Basic command line knowledge helps, but not essential for installation | . Step 1 - Download Hadoop and Hive . Hadoop is easily the most common big data warehouse platform used in industry today and is a must know for any big data job. In short, Hadoop is an open-source software framework used for storing and processing Big Data in a distributed manner. You can download the latest version from here. . Hive is usually added on top of Hadoop to query the data in Hadoop in a SQL like fashion. Hive makes job easy for performing operations like . Data encapsulation | Ad-hoc queries | Analysis of huge datasets | . Hive is slow and generally used for batch jobs only. A much faster version of Hive would be something like Impala, but for home use - it gets the job done. You can download the latest version of Hive here. . . Make sure you download the binary (bin) version and not the source (src) version! Extract the files to /opt . cd ~/Downloads tar -C /opt -xzvf apache-hive-3.1.2-bin.tar.gz tar -C /opt -xzvf hadoop-3.1.3-src.tar.gz . Rename them to hive and hadoop. . cd /opt mv hadoop-3.1.3-src hadoop mv apache-hive-3.1.2-bin hive . Step 2 - Setup Authorized (or Password-less) SSH. . Why do we need to do this? The Hadoop core uses Shell (SSH) to launch the server processes on the slave nodes. It requires a password-less SSH connection between the master and all conencted nodes. Otherwise, you would have to manually go to each node and start each Hadoop process. . Since we are running a local instance of Hadoop, we can save ourselves the hassle of setting up hostnames, SSH keys and adding them to each box. If this were a distributed environment, it would also be best to create a hadoop user, but it’s not necessary for a single node setup and personal use. . The really easy, only suitable for home use, should not be used or done anywhere else way is: . cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys . Now run ssh localhost and you should be able to login without a password. . . To get an idea of what it takes to configure the networking and infrastructure on a distributed environment, this is a great source. . Step 3 - Install Java 8 . One of the most important steps of this tutorial. . . If this is done incorrectly, it will cause a grueling number of hours debugging vague error messages just to realize the problem and solution was so simple. Hadoop has one main requirement and this is Java version 8. Funnily enough, that’s also the Java requirement for Spark, also very important. . sudo apt-get update sudo apt-get install openjdk-8-jdk . Verify the Java version. . java -version . . If for some reason you don’t see the output above, you need to update your default Java version. . sudo update-alternatives --config java . . Choose the number associated with Java 8. . Check the version again. . java -version . . Step 4 - Configure Hadoop + Yarn . Apache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology. At a very basic level it helps Hadoop manage and monitor its workloads. . Initial Hadoop Setup . First let’s set our environment variables. These specifies where the configuration for Hadoop, Spark and Hive is located. . nano ~/.bashrc . Add this to the bottom of your .bashrc file. . export HADOOP_HOME=/opt/hadoop export HADOOP_INSTALL=$HADOOP_HOME export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH export HIVE_HOME=/opt/hive export PATH=$PATH:$HIVE_HOME/bin . Save and exit out of nano CTRL + o, CTRL + x. . Then we need to active these changes by running source ~/.bashrc. You can also close and reopen your terminal to achieve the same result. . Next we need to make some directories and edit permissions. Make the following directories: . sudo mkdir -p /app/hadoop/tmp mkdir -p ~/hdfs/namenode mkdir ~/hdfs/datanode . Edit the permissions for /app/hadoop/tmp, giving it read and write access. . sudo chown -R $USER:$USER /app chmod a+rw -R /app . Config Files . All the Hadoop configuration files are located in /opt/hadoop/etc/hadoop/. . cd /opt/hadoop/etc/hadoop . Next we need to edit the following configuration files: . - core-site.xml - hadoop-env.sh - hdfs-site.xml - mapred-site.xml - yarn-site.xml . core-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt; &lt;description&gt;Parent directory for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS &lt;/name&gt; &lt;value&gt;hdfs://YOUR_IP:9000&lt;/value&gt; &lt;description&gt;The name of the default file system. &lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; . hadoop.tmp.dir: Fairly self explanatory, just a directory for hadoop to store other temp directories fs.defaultFS: The IP and port of your file system to access over the network. It should be your IP so other nodes can connect to it if this were a distributed system. . To find your ip, type ip addr or ifconfig on the command line: . . hadoop-env.sh . Identify the location of the Java 8 JDK, it should be similar or idential to /usr/lib/jvm/java-8-openjdk-amd64/ | Add the following line to hadoop-env.sh: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ | hdfs-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;Default block replication.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file:///home/YOUR_USER/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;file:///home/YOUR_USER/hdfs/datanode&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . dfs.replication: How many nodes to replicate the data on. . dfs.name.dir: Directory for namenode blocks . dfs.data.dir: Directory for the data node blocks . mapred-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt; &lt;value&gt;localhost:54311&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . mapreduce.framework.name: The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn. . mapreduce.jobtracker.address: The host and port that the MapReduce job tracker runs at. If “local”, then jobs are run in-process as a single map and reduce task. . yarn.app.mapreduce.am.env: Yarn map reduce env variable. . mapreduce.map.env: Map reduce map env variable. . mapreduce.reduce.env: Map reduce reduce env variable. . mapreduce.map.memory.mb: Upper memory limit that Hadoop allows to be allocated to a mapper, in megabytes. The default is 512. . mapreduce.reduce.memory.mb: Upper memory limit that Hadoop allows to be allocated to a reducer, in megabytes. The default is 512. . yarn-site.xml . &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;16256&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . yarn.resourcemanager.hostname: The hostname of the RM. Could also be an ip address of a remote yarn instance. . yarn.nodemanager.aux-services: Selects a shuffle service that needs to be set for MapReduce to run. . yarn.nodemanager.resource.memory-mb: Amount of physical memory, in MB, that can be allocated for containers. For reference, I have 64GB of RAM on my machine. If this value is too low, you won’t be able to process large files, getting a FileSegmentManagedBuffer error. . yarn.app.mapreduce.am.resource.mb: This property specify criteria to select resource for particular job. Any nodemanager which has equal or more memory available will get selected for executing job. . yarn.scheduler.minimum-allocation-mb: The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won’t take effect, and the specified value will get allocated at minimum. . Start Hadoop . Before we start Hadoop we have to format the namenode: . hdfs namenode -format . Now we’re good to start Hadoop! Run the following commands: . start-dfs.sh start-yarn.sh . To ensure everything has started run the following commands: . ss -ln | grep 9000 . . jps . . You can now also access the Hadoop web UI at localhost:9870. . . You can also access the Yarn web UI at localhost:8088. . . Step 5 - Setup Hive . Now that we have Hadoop up and running, let’s install Hive on top of it. . First let’s make a directory in Hadoop where our Hive tables are going to be stored. . hdfs dfs -mkdir -p /user/hive/warehouse . Configure permissions. . hdfs dfs -chmod -R a+rw /user/hive . Setup a Metastore . The Hive Metastore is the central repository of Hive Metadata. It stores the meta data for Hive tables and relations (Schema and Locations etc). It provides client access to this information by using metastore service API. There are 3 different types of metastores: . Embedded Metastore: Only one Hive session can be open at a time. | Local Metastore: Multiple Hive sessions, have to connect to an external DB. | Remote Metastore: Multiple Hive sessions, interact with the metastore using Thrift API, better security and scalability. | . To read up on the difference between each type of metastore in more detail, this is a great link. . In this guide we’re going to be setting up a remote metastore using a MySQL DB. . sudo apt update sudo apt install mysql-server sudo mysql_secure_installation . Run the following commands: . sudo mysql . CREATE DATABASE metastore; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;PW_FOR_HIVE&#39;; GRANT ALL ON metastore.* TO &#39;hive&#39;@&#39;%&#39; WITH GRANT OPTION; . Replace PW_FOR_HIVE with the password you want to use for the hive user in MySQL. . Download the MySQL Java Connector: . wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19.tar.gz tar -xzvf mysql-connector-java-8.0.19.tar.gz cd mysql-connect-java-8.0.19 cp mysql-connector-java-8.0.19.jar /opt/hive/lib/ . Edit hive-site.xml . Now edit /opt/hive/conf/hive-site.xml: . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://YOUR_IP:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=UTC&lt;/value&gt; &lt;description&gt;metadata is stored in a MySQL server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;MySQL JDBC driver class&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;user name for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;PW_FOR_HIVE&lt;/value&gt; &lt;description&gt;password for connecting to mysql server&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; . Replace YOUR_IP with the local ip address. Replace PW_FOR_HIVE with the password you initiated for the hive user earlier. . Initialize Schema . Now let’s make MySQL accessible from anywhere on your network. . sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf . Change bind-address to 0.0.0.0. . Restart the service for the changes take effect: sudo systemctl restart mysql.service . Finally, run schematool -dbType mysql -initSchema to initialize the schema in the metastore database. . Start Hive Metastore . hive --service metastore . Testing Hive . First start up Hive from the command line by calling hive. . Let’s create a test table: . CREATE TABLE IF NOT EXISTS test_table (col1 int COMMENT &#39;Integer Column&#39;, col2 string COMMENT &#39;String Column&#39;) COMMENT &#39;This is test table&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS TEXTFILE; . Then insert some test data. . INSERT INTO test_table VALUES(1,&#39;aaa&#39;); . Then we can view the data from the table. . SELECT * FROM test_table; . Step 6 - Setup Spark . Spark is a general-purpose distributed data processing engine that is suitable for use in a wide range of circumstances. On top of the Spark core data processing engine, there are libraries for SQL, machine learning, graph computation, and stream processing, which can be used together in an application. In this tutorial we’re going to setup a standalone Spark cluster using Docker and have it be able to spin up any number of workers. This reasoning behind this is we want to simulate a remote cluster and some of the configuration required for it. . . In a production setting, Spark is usually going to be configured to use Yarn and the resources already allocated for Hadoop. First we need to create the Docker file. We’re going to use Spark version 2.4.4 in this tutorial but you can change it to 2.4.5 if you want the latest version and it also ships with Hadoop 2.7 to manage persistence and book keeping between nodes. In a production setting, Spark is often configured with Yarn to use the existing Hadoop environment and resources, since we only have Hadoop on one node, we’re going to run a spark standalone cluster. To configure Spark to run with Yarn requires minimal changes and you can see the difference in setup here. . Setup Standalone Cluster . nano Dockerfile . # Dockerfile FROM python:3.7-alpine ARG SPARK_VERSION=2.4.4 ARG HADOOP_VERSION=2.7 RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz &amp;&amp; tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C / &amp;&amp; rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz &amp;&amp; ln -s /spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark RUN apk add shell coreutils procps RUN apk fetch openjdk8 RUN apk add openjdk8 RUN pip3 install ipython ENV PYSPARK_DRIVER_PYTHON ipython . Now we want to spin up a Spark master and N number of spark workers. For this we’re going to use docker-compose. . nano docker-compose.yml . version: &quot;3.3&quot; networks: spark-network: services: spark-master: build: . container_name: spark-master hostname: spark-master command: &gt; /bin/sh -c &#39; /spark/sbin/start-master.sh &amp;&amp; tail -f /spark/logs/*&#39; ports: - 8080:8080 - 7077:7077 networks: - spark-network spark-worker: build: . depends_on: - spark-master command: &gt; /bin/sh -c &#39; /spark/sbin/start-slave.sh $$SPARK_MASTER &amp;&amp; tail -f /spark/logs/*&#39; env_file: - spark-worker.env environment: - SPARK_MASTER=spark://spark-master:7077 - SPARK_WORKER_WEBUI_PORT=8080 ports: - 8080 networks: - spark-network . For the master container we’re exposing port 7077 for our applications to connect to and port 8080 for the Spark job UI. For the worker we’re connecting to our Spark master through the environment variables. . For more options to configure the spark worker, we add them to spark-worker.env file. . nano spark-worker.env . SPARK_WORKER_CORES=3 SPARK_WORKER_MEMORY=8G . In this configuration, each worker will use 3 cores and have 8GB of memory. Since my machine has 6 cores, we’re going to start up 2 workers. Change these values so that it is relative to your machine. For example, if your machine only has 16GB of RAM, a good memory value might be 2 or 4GB. For a full list of environment variables and more information on stand alone mode, you can read the full documentation here. If you’re wondering about executor memory, that set when submitting or starting applications. . docker-compose build docker-compose up -d --scale spark-worker=2 . Now spark is up and running and you can view the web UI at localhost:8080! . . Install Spark Locally . On your local machine, or any machine that’s going to be creating or using Spark, Spark needs to be installed. Since we are setting up a remote Spark cluster we have install it from the source. We’re going to use PySpark for this tutorial because I most of the time I use Python for my personal projects. . You can download Spark from here. . . Ensure you download the same version you installed on your master. For this tutorial it&#39;s version 2.4.4 wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz tar -C /opt -xzvf spark-2.4.4-bin-hadoop2.7.tgz . Setup the Spark environment variables, nano ~/.bashrc . export SPARK_HOME=/opt/spark export PATH=$SPARK_HOME/bin:$PATH export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot; export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot; export PYSPARK_PYTHON=python3 . . If you prefer Jupyter Lab, change &#39;notebook&#39;, to &#39;lab&#39; for PYSPARK_DRIVER_PYTHON_OPTS. Config Files . To configure Spark to use our Hadoop and Hive we need to have the config files for both in the Spark config folder. . cp $HADOOP_HOME/etc/hadoop/core-site.xml /opt/spark/conf/ cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml /opt/spark/conf/ . nano /opt/spark/conf/hive-site.xml . &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://YOUR_IP:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.sql.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://YOUR_IP:9000/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; . hive.metastore.uris: Tells Spark to interact with the Hive metastore using the Thrift API. spark.sql.warehouse.dir: Tells Spark where our Hive tables are located in HDFS. . Install PySpark . pip3 install pyspark==2.4.4 or replace 2.4.4 with whatever version you installed on your spark master. . To run PySpark connecting to our distributed cluster run: . pyspark --master spark://localhost:7077, you can also replace localhost with your ip or a remote ip. . This will start up a Jupyter Notebook with the Spark Context pre defined. As a result, we now have a single environment to analyze data with or without Spark. . By default the executor memory is only ~1GB (1024mb). To increase it start pyspark with the following command: . pyspark --master spark://localhost:7077 --executor-memory 7g . There is a 10% overhead per executor in Spark so the most we could assign is 7200mb, but to be safe and have a nice round number we’ll go with 7. . Test Integrations . By default a SparkContext is automatically created and the variable is sc. . To read from our previously created hive table. . from pyspark.sql import HiveContext hc = HiveContext(sc) hc.sql(&quot;show tables&quot;).show() hc.sql(&quot;select * from test_table&quot;).show() . To read a file from Hadoop the command would be: . sparksession = SparkSession.builder.appName(&quot;example-pyspark-read-and-write&quot;).getOrCreate() df = (sparksession .read .format(&quot;csv&quot;) .option(&quot;header&quot;, &quot;true&quot;) .load(&quot;hdfs://YOUR_IP:9000/PATH_TO_FILE&quot;) ) . Practical Hadoop Use Cases . Besides storing data, Hadoop is also utilized as a Feature Store. Let’s say you’re apart of a team or organization and they have multiple models. For each model there is a data pipeline that ingests raw data, computes and transforms the data into features. For one or two models this is perfectly fine, but what if you have multiple models? What if across those models features are being reused (i.e log normalized stock prices)? . Instead of each data pipeline recomputing the same features, we can create a data pipeline that computes the features once and store it in a Feature Store. The model can now pull features from the Feature Store without any redundant computation. This reduces the number of redundant computations and transformations throughout your data pipelines! . . Feature Stores also help with the following issues: . Features are not reused. A common obstacle data scientists face is spending time redeveloping features instead of using previously developed features or ones developed by other teams. Feature stores allow data scientists to avoid repeat work. . | Feature definitions vary. Different teams at any one company might define and name features differently. Moreover, accessing the documentation of a specific feature (if it exists at all) is often challenging. Feature stores address this issue by keeping features and their definitions organized and consistent. The documentation of the feature store helps you create a standardized language around all of the features across the company. You know exactly how every feature is computed and what information it represents. . | There is inconsistency between training and production features. Production and research environments often use different technologies and programming languages. The data streaming in to the production system needs to be processed into features in real time and fed into a machine learning model. . | . If you want to take a look at a Feature Store and get started for free, I recommend StreamSQL. StreamSQL allows you to stream your data from various sources such as HDFS, local file system, Kafka, etc. and create a data pipeline that can feed your model! It has the ability to save the feature store online or on your local HDFS for you to train your models. It also does the service of creating your test (hold out) set for you as well. They have a well documented API and is consistently improving upon it. . Feedback . I encourage all feedback about this post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help. . You can also reach me and follow me on Twitter at @ashtonasidhu. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/big%20data/hadoop/spark/hive/2020/04/17/big-data-setup.html",
            "relUrl": "/markdown/big%20data/hadoop/spark/hive/2020/04/17/big-data-setup.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Intro to Interactive Graph Visualizations",
            "content": "Over the last couple of months I have started to explore more with Graphs and network analysis as it pertains to user behaviour in cyber security. One of the main pain points I found early on was visualizing the network, the node properties and edges in a neat, visually appealing way. Alot of the default plotting options are built on Matplotlib, which is well, let&#39;s just say suboptimal for this type of visualization. . . Instead of playing around with the Matplotlib axes api, I wanted something a little bit more robust, easier to use out of the box, visually appealing and neat while maximizing the possible amount of information that can be displayed on the graph. . In comes my favourite visualization library Plotly. Plotly already had some documentation on how to visualize graphs, but it was still a fairly lengthy process. The goal was to find (or create) a uniform api that follows the .plot standard, with some customizability, but for graphs. Hence, why I made this little package called Interactive Graph Visualizations (igviz). . . Note: Currently, only Networkx is supported but the next step is expanding it to Apache&#8217;s Graphx. . To install run pip install igviz . import networkx as nx import igviz as ig . Create a random graph for demonstration purposes and assign every node a property called prop and every edge a property called &quot;edge_prop&quot; and make the values 12 and 3. . G = nx.random_geometric_graph(200, 0.125) nx.set_node_attributes(G, 12, &quot;prop&quot;) nx.set_edge_attributes(G, 3, &quot;edge_prop&quot;) . Now we plot! . Note: This also works with Directed and Multigraphs. Directed Graphs will show the arrows from node to node. . The Basics . . Tip: All the example notebooks can be found here. By default, the nodes are sized and coloured by the degree of itself. The degree of a node is just the number of edge&#39;s (lines connecting 2 nodes) it has. You can see the degree of the node by hovering over it! . fig = ig.plot(G) fig.show() . . . . Tip: When it comes to customizability, you can change the how the nodes are sized (size_method) to either be static, based off a node&#8217;s property or your own custom sizing method. . Tip: The color&#8217;s can be changed (color_method) to be a static color (hex, plain text, etc.), based off a node&#8217;s property or your own custom color method. . There are more options, and more to come, but those are the ones that greatly impact the visualization and information of your graph. . Customizing the Graph Appearance . Here all the nodes are set to the same size and the colour is set to a light red while displaying the prop property on hover. . . Tip: To display Node&#8217;s properties when you hover it, specify a list for the node_text parameter of node properties you want to have displayed. By default only degree is shown. . fig = ig.plot( G, # Your graph title=&quot;My Graph&quot;, size_method=&quot;static&quot;, # Makes node sizes the same color_method=&quot;#ffcccb&quot;, # Makes all the node colours black, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node annotation_text=&quot;Visualization made by &lt;a href=&#39;https://github.com/Ashton-Sidhu/plotly-graph&#39;&gt;igviz&lt;/a&gt; &amp; plotly.&quot;, # Adds a text annotation to the graph ) fig.show() . . . Here, the sizing and color method is based off the prop property of every node as well as we&#39;re displaying the prop property on hover. . fig = ig.plot( G, title=&quot;My Graph&quot;, size_method=&quot;prop&quot;, # Makes node sizes the size of the &quot;prop&quot; property color_method=&quot;prop&quot;, # Colors the nodes based off the &quot;prop&quot; property and a color scale, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node ) fig.show() . . . To add your own sizing and color methods, pass in a list of a color or size pertaining to each node in the graph to the size_method and color_method parameters. . Tip: To change the colorscale, change the colorscale parameter! . color_list = [] sizing_list = [] for node in G.nodes(): size_and_color = G.degree(node) * 3 color_list.append(size_and_color) sizing_list.append(size_and_color) fig = ig.plot( G, title=&quot;My Graph&quot;, size_method=sizing_list, # Makes node sizes the size of the &quot;prop&quot; property color_method=color_list, # Colors the nodes based off the &quot;prop&quot; property and a color scale, node_text=[&quot;prop&quot;], # Adds the &#39;prop&#39; property to the hover text of the node ) fig.show() . . . Labels . You can also add labels to both nodes and edges as well as add hover text to the edges based on their attributes. . ig.plot( G, node_label=&quot;prop&quot;, # Display the &quot;prop&quot; attribute as a label on the node node_label_position=&quot;top center&quot;, # Display the node label directly above the node edge_text=[&quot;edge_prop&quot;], # Display the &quot;edge_prop&quot; attribute on hover over the edge edge_label=&quot;edge_prop&quot;, # Display the &quot;edge_prop&quot; attribute on the edge edge_label_position=&quot;bottom center&quot;, # Display the edge label below the edge ) . . . Layouts . You can change the way your graph is oragnized and laid out by specifying a type of layout. Networkx comes with predefined layouts to use and we can apply them through layout. . By default, igviz looks for the pos node property and if it doesn&#39;t exist it will default to a random layout. . The supported layouts are: . random (default): Position nodes uniformly at random in the unit square. For every node, a position is generated by choosing each of dim coordinates uniformly at random on the interval [0.0, 1.0). . | circular: Position nodes on a circle. . | kamada: Position nodes using Kamada-Kawai path-length cost-function. . | planar: Position nodes without edge intersections, if possible (if the Graph is planar). . | spring: Position nodes using Fruchterman-Reingold force-directed algorithm. . | spectral: Position nodes using the eigenvectors of the graph Laplacian. . | spiral: Position nodes in a spiral layout. . | . fig = ig.plot( G, title=&quot;My Graph&quot;, layout=&quot;kamada&quot; ) fig.show() . . . To add your own pos property you can set it via the nx.set_node_attributes function. . pos_dict = { 0: [1, 2], # X, Y coordinates for Node 0 1: [1.5, 3], # X, Y coordinates for Node 1 ... } nx.set_node_attributes(G, pos_dict, &quot;pos&quot;) fig = ig.plot(G) fig.show() . Feedback . I encourage all feedback about this post or Igviz. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. . . Tip: If you like the project, give it a star! .",
            "url": "https://ashton-sidhu.github.io/blog/jupyter/graph/visualization/2020/03/27/intro-to-igviz.html",
            "relUrl": "/jupyter/graph/visualization/2020/03/27/intro-to-igviz.html",
            "date": " • Mar 27, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
            "content": "As a data scientist, rapid experimentation is extremely important. If an idea doesn’t work it’s best to fail quickly and find out sooner rather than later. . When it comes to modelling, rapid experimentation is already fairly simple. All model implementations follow the same API interface so all you have to do is initialize the model and train it. The problem comes now when you have to interpret, track, and compare each model. . Do you make a large notebook with all your models and scroll up and down or use a table of contents to see the results of different models? Do you create a different notebook for each model and then flip back and forth between notebooks? How do you track iterations of the models if you start tweaking the parameters? Where do you store artifacts to revisit at a later date or for further analysis? . I’m going to demonstrate a way to address these problems by training multiple models, each with 1 line of code, view the overall results easily, analyze the models, interpret the models, track the models in MLFlow and serve them using Aethos. . Prepping the Data . We’ll use Aethos to quickly prep the data. For more information on how to analyze and transform your datasets with Aethos, check out my previous blog post here. Load the Titanic training data from the Aethos repo. . import aethos as at import pandas as pd data = pd.read_csv(&#39;https://raw.githubusercontent.com/Ashton-Sidhu/aethos/develop/examples/data/train.csv&#39;) . Pass the data into Aethos. . df = at.Data(data, target_field=&#39;Survived&#39;) . . The focus of this post is modelling so let’s quickly preprocess the data. We’re going to use the Survived, Pclass, Sex, Age, Fare and Embarked features. insert here . df.drop(keep=[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]) df.standardize_column_names() . Replace the missing values in the Age and the Embarked columns. . df.replace_missing_median(&#39;age&#39;) df.replace_missing_mostcommon(&#39;embarked&#39;) . Normalize the values in the Age and Fare columns and One Hot Encode the Sex, Pclass and Embarked features. . df.onehot_encode(&#39;sex&#39;, &#39;pclass&#39;, &#39;embarked&#39;, keep_col=False) df.normalize_numeric(&#39;fare&#39;, &#39;age&#39;) . . With Aethos, the transformer is fit to the training set and applied to the test set. With just one line of code both your training and test set have been transformed. . Modelling . To train Sklearn, XGBoost, LightGBM, etc. models with Aethos, first transition from the data wrangling object to the Model object. . model = at.Model(df) . The model object behaves the same way as the Data object so if you have data that has already been processed, you can initiate the Model object the same way as the Data object. . Next, we’re going to enable experiment tracking with MLFlow. . at.options.track_experiments = True . Now that everything is set up, training a model and getting predictions is as simple as this: . lr = model.LogisticRegression(C=0.1) . . To train a model with the optimal parameters using Gridsearch, specify the gridsearch parameter when initializing the model. . lr = model.LogisticRegression(gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001) . . This will return the model with the optimal parameters defined by your Gridsearch scoring method. . Finally, if you want to cross validate your models there are a few options: . lr = model.LogisticRegression(cv=5, C=0.001) . This will perform 5-fold cross validation on your model and display the mean score as well as the learning curve to help gauge data quality, over-fitting, and under-fitting. . . You can also use it with Gridsearch: . lr = model.LogisticRegression(cv=5, gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001) . This will first use Gridsearch to train a model with the optimal parameters and then cross validate it. Currently supported cross validation methods are k-fold and stratified k-fold. . . To train multiple models at once (in series or in parallel) specify the run parameter when defining your model. . lr = model.LogisticRegression(cv=5, gridsearch={&#39;C&#39;: [0.01, 0.1]}, tol=0.001, run=False) . Let’s queue up a few more models to train: . model.DecisionTreeClassification(run=False) model.RandomForestClassification(run=False) model.LightGBMClassification(run=False) . You can view queued and trained models by running the following: . model.list_models() . . To now run all queued models, by default in parallel: . dt, rf, lgbm = model.run_models() . You can now go grab a coffee or a meal while all your models are trained simultaneously! . Analyzing Models . Every model you train by default has a name. This allows you to train multiple versions of the same model with the same model object and API, while still having access to each individual model’s results. You can see each model’s default name in the function header by pressing Shift + Tab in Jupyter Notebook. . . You can also change the model name by specifying a name of your choosing when initializing the function. . First, let’s compare all the models we’ve trained against each other: . model.compare_models() . . You can see every models’ performance against a wide variety of metrics. In a data science project there are predefined metrics you want to compare models against (if you don’t, you should). You can specify those project metrics through the options. . at.options.project_metrics = [&#39;Accuracy&#39;, &#39;Balanced Accuracy&#39;, &#39;Zero One Loss&#39;] . Now when you compare models, you’ll only see the project metrics. . model.compare_models() . . You can also view the metrics just for a single model by running the following: . dt.metrics() # Shows metrics for the Decision Tree model rf.metrics() # Shows metrics for the Random Forest model lgbm.metrics() # Shows metrics for the LightGBM model . You can use the same API to see each models’ RoC Curve, Confusion matrix, the index of misclassified predictions, etc. . dt.confusion_matrix(output_file=&#39;confusion_matrix.png&#39;) # Confusion matrix for the Decision Tree model . . rf.confusion_matrix(output_file=&#39;confusion_matrix.png&#39;) # Confusion matrix for the random forest model . . Interpreting Models . Aethos comes equipped with automated SHAP use cases to interpret each model. You can view the force, decision, summary, and dependence plot for any model — each with customizable parameters to satisfy your use case. . By specifying the output_file parameter, Aethos knows to save and track this artifact for a specific model. . lgbm.decision_plot(output_file=&#39;decision_plot.png&#39;) . . lgbm.force_plot(output_file=&#39;force_plot.png&#39;) . . lgbm.shap_get_misclassified_index() # [2, 10, 21, 38, 43, 57, 62, 69, 70, 85, 89, 91, 96, 98, 108, 117, 128, 129, 139, 141, 146, 156, 165, 167, 169] lgbm.force_plot(2, output_file=&#39;force_plot_2.png&#39;) . . lgbm.summary_plot(output_file=&#39;summary_plot.png&#39;) . . lgbm.dependence_plot(&#39;fare&#39;, output_file=&#39;dep_plot.png&#39;) . . For a more automated experience, run: . lgbm.interpret_model() . . This displays an interactive dashboard where you can interpret your model’s prediction using LIME, SHAP, Morris Sensitivity, etc. . Viewing Models in MLFlow . As we were training models and viewing results, our experiments were automatically being tracked. If you run aethos mlflow-ui from the command line, a local MLFlow instance will be started for you to view the results and artifacts of your experiments. Navigate to localhost:10000. . . Each of your models and any saved artifacts are tracked and can be viewed from the UI, including parameters and metrics! To view and download model artifacts, including the pickle file for a specific model, click on the hyperlink in the date column. . . You get a detailed breakdown of the metrics for the model as well as the model parameters and towards the bottom, you will see all your saved artifacts for a specific model. . . Note: Cross validation learning curves and mean score plots are always saved as artifacts. . To change the name of the experiment ( by default it is my-experiment) specify the name when initiating the Model object ( exp_name), otherwise every model will get added to my-experiment. . . Serving Models . Once you’ve decided on a model you want to serve for predictions, you can easily generate the required files to serve the model with a RESTful API using FastAPI and Gunicorn. . dt.to_service(&#39;titanic&#39;) . . If you’re familiar with MLFlow, you can always use it to serve your models as well. Open a terminal where the deployment files are being kept. I recommend moving these files to a git repository to version control both the model and served files. . Follow the instructions to build the docker container and then run it detached. . docker build -t titanic:1.0.0 ./ docker run -d --name titanic -p 1234:80 titanic:1.0.0 . Now navigate to localhost:1234/docs to test your API. You can now serve predictions by sending POST requests to 127.0.0.1:1234/predict. . . . Now one big thing to note, this is running a default configuration and should not be used in production without securing it and configuring the server for production use. In the future, I will add these configurations myself so you can more or less “throw” a model straight into production with minimal configuration changes. . This feature is still in its infancy and one that I will actively continue to develop and improve. . What Else Can You Do? . While this post is a semi- comprehensive guide of modelling with Aethos, you can also run statistical tests such as T-Test, Anovas, use pretrained models such as BERT and XLNet for sentiment analysis and question answering, perform extractive summarization with TextRank, train a gensim LDA model, as well as clustering, anomaly detection and regression models. . The full example can be seen here! . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/aethos/2020/03/14/aethos-modelling.html",
            "relUrl": "/markdown/aethos/2020/03/14/aethos-modelling.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Aethos — A Data Science Library to Automate your Workflow",
            "content": "As a Data Scientist in industry, there are a few pain points that I’m sure a lot of other data scientists can relate to: . Copy and pasting code from previous projects to your current project or from notebook to notebook is frustrating. | For the same task, each data scientist will do it their own way. | Managing experiments with notebook names gets unorganized very quickly and structuring data science projects can be a mess. | Documenting your process while you’re in the middle of your analysis often disrupts your flow. | Version controlling models, transforming it into a service, and developing a data pipeline after a model has been trained. | There are tons of different libraries coming out that each fill a niche of Data Science &amp; ML and learning a new API (especially for deep learning and NLP) or reading documentation (if there is any) every time gets a little tiresome. Ironic, I know. | Making just even semi-appealing visualizations is often time consuming and sometimes difficult. | This is why I made Aethos. . Aethos is a Python library of automated Data Science techniques and use cases from missing value imputation, to NLP pre-processing, feature engineering, data visualizations and all the way to modelling and generating code to deploy your model as a service and soon, your data pipeline. . Installation . Let’s install Aethos like any other Python package: . pip install aethos . Once Aethos is installed, we can already do a couple of things. We can install all the required NLP corpora, . aethos install-corpora . install extensions such as QGrid for a better experience when interacting with your data . aethos enable-extensions . and create a Data Science project with the full folder structure to house source code, models, data and experiments. . aethos create . Getting Started . First, let’s import the necessary libraries: . import aethos as at import pandas as pd . Before we start, let’s also configure some options that will make our life easier. For myself, I often write a report of the steps I took during my analysis and why I did them to better communicate my process to other data scientists on the team or to myself if I come back to an experiment after a prolonged length of time. Instead of writing reports manually, Aethos will automatically write the report as a text file as we go through our analysis (all reports are saved in %HOME%/.aethos/reports/). To enable Aethos to write the report as a word doc, we just have to enable the option: . at.options.word_report = True at.options.interactive_table = True . The interactive_table option will display our data using the itables library, a personal favorite of mine as it has a client side search feature. . Now let’s load our data. Aethos takes in data in the form of a pandas dataframe so data is loaded just like if you were working with pandas. We’re going to use the titanic dataset for this example. . data = pd.read_csv(&#39;titanic.csv&#39;) . To use Aethos, we just have to pass the dataframe into the Aethos Data object. . df = at.Data(data, target_field=&#39;Survived&#39;, report_name=&#39;titanic&#39;) df . . Couple of things to note about what just happened: . Since this is a supervised learning problem we specified the column we want to predict (‘Survived’). | By default since we did not pass any test data to the Data object, Aethos automatically splits the data given into a train and test set. This can be turned off by setting split=False when initializing the Data object. | By default, the split percentage is 20. This can be changed by setting test_split_percentage to a float between 0 and 1 (by default it is 0.2). | We specified the report name to titanic, since we specified we want a Word Doc created, a titanic.txt and a titanic.docx will be created in %HOME%/.aethos/reports/. | . Let’s get started with some basic analysis. . Analysis . Using the Data object is just like working with a Pandas DataFrame. . df[&#39;Age&#39;] # returns a Pandas series of the training data in the Age column df[df[&#39;Age&#39;] &gt; 25] # returns a Pandas DataFrame of the the training data where Age is &gt; 25. df.nunique() # You can even run pandas functions on the Data object, however they only operate on your training data. . To add new columns is the same as doing it in pandas. When adding a new column it will add the new data to the dataset (train or test if data is split), based on the length of the dataset. You do, however, have the ability to specify which dataset to add it to as well: . # based off the length of some iterable it will add to the train set or test set (if data is split). df[`new_col_name`] = `some_iterable` # You can also specify what dataset to add it to as well df.x_train # pandas dataframe of the training set df.x_test # pandas dataframe of the test set df.x_train[`new_col`] = `some_iterable` df.x_test[`new_col`] = `some_iterable` . Aethos offers a few options to get started by getting a holistic view of your training data. The first one is one that you are probably familiar with. Aethos’ describe function extends pandas to provide a little extra information. . df.describe() . . We can also get more detailed information about each column, all of this is available through the pandas-summary library. . df.describe_column(&#39;Age&#39;) . . The command also provides a histogram of the data. Each statistical value can be accessed by referencing its name. . df.describe_column(&#39;Age&#39;) [&#39;iqr&#39; ] . The other option is to generate an EDA report available through pandas-profiling. . df.data_report() . . We can also generate histograms, pairplots and jointplots all with one line of code (Note: image sizes were scaled to a better fit for this article and are configurable). . df.jointplot(&#39;Age&#39;, &#39;Fare&#39;, kind=&#39;hex&#39;, output_file=&#39;age_fare_joint.png&#39;) df.pairplot(diag_kind=&#39;hist&#39;, output_file=&#39;pairplot.png&#39;) df.histogram(&#39;Age&#39;, output_file=&#39;age_hist.png&#39;) . . . Jointploit, Pairplot and histogram (L to R) . We can also view information about missing values in both datasets at any time. . df.missing_values . . Let’s deal with the missing values in the Cabin, Age and Embarked columns. . For the purpose of this article we will replace missing values in the Embarked column with the most common value and missing values in the Age column with the median value. We will then drop the Cabin column. . df.replace_missing_mostcommon(&#39;Embarked&#39;) . A lot just happened in a few lines of code. Let’s start with the imputation. . Aethos uses established packages (sci-kit learn, nltk, gensim, etc.) to do all the analysis, in this case, imputation. | To avoid data leakage all the analytical techniques are fit to the training set and then applied to the test set. | In general when there is a test set, whatever is done to the training set will be applied to the test set. | When you run a technique a small description of the technique is written to the report. | All of this is consistent for every Aethos analytical technique and can be viewed in the source code here (this function is in cleaning/clean.py). | . df.replace_missing_median(&#39;Age&#39;) df.drop(&#39;Cabin&#39;) . Let’s also drop the Ticket, Name, PClass, PassengerId, Parch, and SibSp columns. . df.drop(&#39;Ticket&#39;, &#39;Name&#39;, &#39;PClass&#39;, &#39;PassengerId&#39;, &#39;Parch&#39;, &#39;SibSp&#39;) . This isn’t a post about why you should try avoiding the .apply function in pandas but in the interest of API coverage and consistency, you can use .apply with Aethos as well. Specify a function and an output column name and you’re good to go. . The two big differences are that the whole DataFrame is passed into the function and the Swifter library is used to parallelize the running of the function, decreasing run time. . def get_person(data): age, sex = data[&#39;Age&#39;], data[&#39;Sex&#39;] return &#39;child&#39; if age &lt; 16 else sexdf.apply(get_person, &#39;Person&#39;) . . Just like with any other Aethos method the function get_person is applied to both the train and test set. Aethos provides an optimization when using .apply but it’s still best to use vectorization when possible. . We can now drop the Sex column. . df.drop(&#39;Sex&#39;) . We need to transform Embarked and Person to numeric variables and for this we’ll just use one-hot encoding for simplicity. We also want to drop the original columns which we specify with the keep_col parameter. Since we’re using scikit-learn’s OneHotEncoder class, we can pass keyword arguments to its’ constructor as well. . df.onehot_encode(&#39;Embarked&#39;, &#39;Person&#39;, keep_col=False, drop=None) . . Reporting . A couple of things to keep in mind about the current reporting feature is that it records and writes in the order the code is executed and that it currently does not caption any images. We can also write to the report at any time by doing the following: . df.log(`write_something_here`) . Conclusion . Aethos is standardized API that allows you to run analytical techniques, train models and more, with a single line of code. What I demonstrated is just the tip of the iceberg of the analytical techniques you can do with Aethos. There are over 110 automated techniques and models in Aethos with more coming soon. . In the subsequent blog posts I will go over training models and viewing results with Aethos. For more detail, I recommend reading the Usage section on Github or the documentation. . Coming Soon . This is just an introduction to Aethos but new features are coming soon! For a full roadmap you can view it here. . In the near future you can expect: . Experiment management with MLFlow | Improved existing visualizations with a better API | More automated visualizations | More automated techniques (dimensionality reduction, feature engineering) | Pre-trained models (GPT-2, BERT, etc.) | . Feedback . I encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com. . Any bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started. .",
            "url": "https://ashton-sidhu.github.io/blog/markdown/aethos/2020/03/14/aethos-intro.html",
            "relUrl": "/markdown/aethos/2020/03/14/aethos-intro.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashton-sidhu.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . Hi! My name is Ashton Sidhu and I’m currently a Data Scientist focused in the cyber security area. I’m an engineer at heart and have both a Bachelors of Applied Science and Engineering as well as a Master’s of Engineering focused in Information Systems as well as Predictive and Perscriptive analytics. . I pride myself on being a swiss army knife engineer and being able to be self sufficient. I’ve been a software developer leading and delivering production releases of applications, I’ve also done ETL work, ingesting raw data and storing it into a Data Lake or Warehouse. I’ve also automated the deployment of infrastructure as well as deploy machine learning models into applications as well as production. I’ve always been naturally curious and had this drive be able to learn and do everything. It keeps me motivated to this day and allows me to experiment more freely as well as building my own software, application or product end to end. . Hobbies . Sports . Sports have always been a big part of my life since I was a kid, from playing high level baseball or sitting at home watching the Toronto Maple Leafs with the entire family (I used to cry when I was a kid when they lost in the playoffs). I played Baseball at the city level and pretty much everything else recreationally since I was a kid. . Nowadays, I find watching sports during the regular season difficult. As ironic as it may be, I think analytics has taken away the watchability of some sports. With hockey specifically, the game in the past used to be alot more free flowing and less structured compared to today’s games. I understand that this was going to be the natural evolution of sports as more data became accessible, but nonetheless, not the same as it used to be. Wow, am I getting old? . Played . Baseball: INF + Pitcher Hockey (Ball): Goalie + Defence Football: TE Soccer: Goalie Basketball: SG + SF Favorite Teams Baseball: Blue Jays Hockey: Toronto Maple Leafs Football: Oakland Raiders, Green Bay Packers (Aaron Rodgers), Kansas City Chiefs (Patrick Mahomes) Soccer: Barcelona, Bayern Munich, Liverpool Basketball: Toronto Raptors, Lebron James . Tech . I experiment with a lot of tech on my free time, specifically automation. I’ve set up my own home infrastructure to automate and aggegrate a lot of day to day tasks. I also experiment with infrastructure (I’m afraid to leave machines running on the cloud and get wopped with a huge bill.) and run a lot of my own on prem solutions. I run my own Spark cluster, data storages (Hadoop, Cassandra, ELK, etc.), Jupyter Server, Zeppelin, Kafka, etc for any analytics, monitoring, or ETL processes that I have throughout my place. . I also experiment with machine learning, specifically in the cyber security space, both on the defensive and offensive side. Those ideas can be seen and replicated through the blog posts on my site, so check them out. . Other than analytics, I also do some Hacker One as well as Hack the Box. . Gaming . I’ve been gaming since I was a kid on the original Gameboy playing Pokemon. I started as a PC Gamer with Runescape and Maplestory. Then moved to the PS3 to play CoD MW2 and then when League of Legends came out and took over the industry when I was in first year university .. well ya. I picked up CS: GO around 3rd year university, played in some amateur tournaments and took home some money. Then I retired from my E-sports career shortly after while on top… Anyways, now I just game casually on my free time. . Guild Wars 2 CS: GO Civilization 6 . Music . Who doesn’t like music? It’s fair to say that music plays a portion in everyone’s life whether it’s art or recreational. For me, studying music in university was my first option. When I first started even just in choir, I was terrible, always off key and couldn’t keep a rhythm to save my life. Then I picked up the Alto Sax, which I was just as terrible as when I first started. I was the last kid in my class to even just get a sound out of it. . Around Grade 9 is when everything changed and it turns out I had some skill and a passion for it. I made the decision that I wanted to study Performance Music in university. At this point I had made my mind up that I wanted to go the University of Toronto because at the time it was the best school in Canada (it might still be, I don’t keep up with the rankings anymore.). If I didn’t get into U of T for music, I was going to go there for Engineering Science. I managed to get the professor who teaches Alto Sax at U of T for Performance Music as my teacher and ultimately didn’t get in. . Since then, I’ve performed in 1 or 2 concerts and was the Alto Sax player for the engineering production Skule Nite for 5 years. Some of the greatest memories of my life. . Fun Facts . My first dream job was a Hockey commentator. I used to play mini sticks by myself and commentate the games. | I once serenaded McMaster university with “Careless Whisper”. There’s a video of it on the internet somewhere. | I was once a ranked chess player in Ontario. Used to compete in tournaments. | Latin music is my favourite genre. Although once I had an EDM phase. | Turns out, I’ve never travelled to a different timezone. It’s still one of my goals to see the world. | .",
          "url": "https://ashton-sidhu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashton-sidhu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}